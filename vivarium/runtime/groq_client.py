"""
import self_observer
Groq API Client - Drop-in replacement for CLI-based execution.

Provides LocalInferenceEngine that routes to Groq's API with:
- Adaptive model selection (8B for simple, 70B for complex)
- Cost tracking
- Rate limiting awareness
- Fail-safe error handling
"""

import os
import json
import time
import threading
from typing import Dict, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass

# Groq SDK import
try:
    from groq import Groq
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False
    print("[GROQ] Warning: groq package not installed. Run: pip install groq")


@dataclass
class GroqModel:
    """Model configuration for Groq."""
    model_id: str
    display_name: str
    input_cost_per_1m: float  # USD per 1M input tokens
    output_cost_per_1m: float  # USD per 1M output tokens
    context_window: int
    max_completion: int
    tokens_per_second: int


# Groq model registry
GROQ_MODELS = {
    # GPT-OSS 120B - The big one, always use this
    # Pricing: https://console.groq.com/docs/models
    "openai/gpt-oss-120b": GroqModel(
        model_id="openai/gpt-oss-120b",
        display_name="GPT-OSS 120B",
        input_cost_per_1m=0.15,
        output_cost_per_1m=0.60,
        context_window=131072,
        max_completion=32768,
        tokens_per_second=300
    ),
    # GPT-OSS 20B
    "openai/gpt-oss-20b": GroqModel(
        model_id="openai/gpt-oss-20b",
        display_name="GPT-OSS 20B",
        input_cost_per_1m=0.075,
        output_cost_per_1m=0.30,
        context_window=131072,
        max_completion=4096,
        tokens_per_second=400
    ),
    # COMPOUND: Auto-selects model (keeping for backwards compat)
    "groq/compound": GroqModel(
        model_id="groq/compound",
        display_name="Groq Compound (Auto-Select)",
        input_cost_per_1m=0.15,
        output_cost_per_1m=0.60,
        context_window=131072,
        max_completion=32768,
        tokens_per_second=400
    ),
    # COMPOUND-MINI: Fast auto-select, single tool call, 3x lower latency
    "groq/compound-mini": GroqModel(
        model_id="groq/compound-mini",
        display_name="Groq Compound Mini (Fast Auto)",
        input_cost_per_1m=0.10,
        output_cost_per_1m=0.40,
        context_window=131072,
        max_completion=16384,
        tokens_per_second=600
    ),
    # Fast/cheap executor (Haiku equivalent)
    "llama-3.1-8b-instant": GroqModel(
        model_id="llama-3.1-8b-instant",
        display_name="Llama 3.1 8B Instant",
        input_cost_per_1m=0.05,
        output_cost_per_1m=0.08,
        context_window=131072,
        max_completion=8192,
        tokens_per_second=560
    ),
    # Versatile strategist (Opus equivalent)
    "llama-3.3-70b-versatile": GroqModel(
        model_id="llama-3.3-70b-versatile",
        display_name="Llama 3.3 70B Versatile",
        input_cost_per_1m=0.59,
        output_cost_per_1m=0.79,
        context_window=131072,
        max_completion=32768,
        tokens_per_second=280
    ),
    # Safety-focused model
    "llama-guard-3-8b": GroqModel(
        model_id="llama-guard-3-8b",
        display_name="Llama Guard 3 8B",
        input_cost_per_1m=0.20,
        output_cost_per_1m=0.20,
        context_window=8192,
        max_completion=1024,
        tokens_per_second=1200
    ),
}

# Per-model optimal API settings (from Groq docs: console.groq.com/docs/prompting)
# Temperature: 0 = deterministic, 0.2 = factual, 0.7 = creative, 0.8 = copywriting
# Use temp OR top_p, not both (Groq recommends one)
GROQ_MODEL_SETTINGS: Dict[str, Dict[str, Any]] = {
    "openai/gpt-oss-120b": {"temperature": 0.7, "top_p": None, "max_tokens": 4096},
    "openai/gpt-oss-20b": {"temperature": 0.7, "top_p": None, "max_tokens": 4096},
    "groq/compound": {"temperature": 0.7, "top_p": None, "max_tokens": 8192},
    "groq/compound-mini": {"temperature": 0.7, "top_p": None, "max_tokens": 8192},
    "llama-3.1-8b-instant": {"temperature": 0.3, "top_p": None, "max_tokens": 4096},
    "llama-3.3-70b-versatile": {"temperature": 0.7, "top_p": None, "max_tokens": 4096},
    "llama-guard-3-8b": {"temperature": 0.0, "top_p": None, "max_tokens": 1024},
}
# Task-type overrides (override model defaults when task type is explicit)
# From Groq docs: factual 0.2, creative 0.8, brainstorming 0.7, code 0.3, extraction 0.0
GROQ_TASK_OVERRIDES: Dict[str, Dict[str, Any]] = {
    "deterministic": {"temperature": 0.0},
    "factual": {"temperature": 0.2},
    "creative": {"temperature": 0.8},
    "brainstorming": {"temperature": 0.7},
    "code": {"temperature": 0.3},
    "extraction": {"temperature": 0.0},
}


def _resolve_model_settings(
    model_id: str,
    task_type: Optional[str] = None,
    temperature: Optional[float] = None,
    top_p: Optional[float] = None,
    max_tokens: Optional[int] = None,
) -> Dict[str, Any]:
    """Resolve temperature, top_p, max_tokens from model defaults + task type + explicit overrides."""
    model_settings = GROQ_MODEL_SETTINGS.get(model_id, {"temperature": 0.7, "top_p": None, "max_tokens": 4096})
    out = dict(model_settings)
    if task_type and task_type in GROQ_TASK_OVERRIDES:
        out.update(GROQ_TASK_OVERRIDES[task_type])
    if temperature is not None:
        out["temperature"] = temperature
    if top_p is not None:
        out["top_p"] = top_p
    if max_tokens is not None:
        out["max_tokens"] = max_tokens
    return out


# Model aliases - DEFAULT is GPT-OSS 120B
MODEL_ALIASES = {
    "auto": "openai/gpt-oss-120b",     # Default: GPT-OSS 120B
    "default": "openai/gpt-oss-120b",  # Explicit default
    "120b": "openai/gpt-oss-120b",     # Direct 120B access
    "gpt-oss": "openai/gpt-oss-120b",  # Shorthand
    "compound": "groq/compound",       # Legacy compound (auto-select)
    "compound-mini": "groq/compound-mini",
    "haiku": "groq/compound-mini",
    "sonnet": "openai/gpt-oss-120b",
    "opus": "openai/gpt-oss-120b",
    "fast": "groq/compound-mini",
    "smart": "openai/gpt-oss-120b",
    "guard": "llama-guard-3-8b",
    "8b": "llama-3.1-8b-instant",
    "70b": "llama-3.3-70b-versatile",
}

DEFAULT_SYSTEM_PROMPT = (
    "You are an expert software engineer executing tasks precisely. "
    "Follow instructions exactly."
)


class GroqInferenceEngine:
    """
    Local inference engine using Groq API.

    Replaces CLI calls with Groq API requests.
    Supports adaptive model selection based on task complexity.
    """

    def __init__(self, api_key: Optional[str] = None):
        """
        Initialize Groq client.

        Args:
            api_key: Groq API key. Falls back to GROQ_API_KEY env var.
        """
        self.api_key = api_key or os.environ.get("GROQ_API_KEY")

        if not self.api_key:
            raise ValueError(
                "GROQ_API_KEY not found. Set it via environment variable or pass to constructor.\n"
                "Get your key at: https://console.groq.com/keys"
            )

        if not GROQ_AVAILABLE:
            raise ImportError("groq package required. Install with: pip install groq")

        self.client = Groq(api_key=self.api_key)
        self.total_cost = 0.0
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.request_count = 0

        # Rate limiting state (thread-safe)
        # Groq free tier: 30 requests/minute = 2 seconds between requests
        self._rate_lock = threading.Lock()
        self.last_request_time = 0
        self.min_request_interval = 2.0  # 2 seconds between requests (Groq rate limit)

    def _resolve_model(self, model: str) -> str:
        """Resolve model alias to actual Groq model ID."""
        # Check if it's an alias
        if model.lower() in MODEL_ALIASES:
            return MODEL_ALIASES[model.lower()]

        # Check if it's a direct model ID
        if model in GROQ_MODELS:
            return model

        # Check for partial matches (e.g., "llama-3.1-8b" -> "llama-3.1-8b-instant")
        for model_id in GROQ_MODELS:
            if model.lower() in model_id.lower():
                return model_id

        # Default to fast model
        print(f"[GROQ] Unknown model '{model}', defaulting to openai/gpt-oss-120b")
        return "openai/gpt-oss-120b"

    def _select_model_for_complexity(self, base_model: str, complexity_score: float) -> str:
        """
        Model selection - NOW DELEGATED TO GROQ COMPOUND.

        Groq's compound system automatically selects from GPT-OSS 120B,
        Llama 4 Scout, or Llama 3.3 70B based on task complexity.
        We just pass "groq/compound" and let it handle the rest.

        Args:
            base_model: Requested model (ignored if using compound)
            complexity_score: Task complexity (ignored - Groq handles this)

        Returns:
            Selected model ID
        """
        resolved = self._resolve_model(base_model)

        # If they explicitly asked for a specific model (8b, 70b, guard), use it
        if resolved in ["llama-3.1-8b-instant", "llama-3.3-70b-versatile", "llama-guard-3-8b"]:
            return resolved

        # Default: GPT-OSS 120B
        return "openai/gpt-oss-120b"

    def _rate_limit_wait(self):
        """Enforce rate limiting between requests (thread-safe)."""
        with self._rate_lock:
            elapsed = time.time() - self.last_request_time
            if elapsed < self.min_request_interval:
                time.sleep(self.min_request_interval - elapsed)
            self.last_request_time = time.time()

    def execute(
        self,
        prompt: str,
        model: str = "openai/gpt-oss-120b",  # DEFAULT: GPT-OSS 120B
        complexity_score: float = 0.0,  # Ignored - Groq Compound handles this automatically
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        timeout: int = 600,
        seed: Optional[int] = None,
        task_type: Optional[str] = None,
        system_prompt: Optional[str] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        Execute a prompt using Groq API.

        Args:
            prompt: The prompt to execute
            model: Model name or alias
            complexity_score: Task complexity (0.0-1.0) for adaptive selection
            max_tokens: Maximum tokens in response
            temperature: Sampling temperature
            timeout: Request timeout in seconds
            system_prompt: Optional system instruction override

        Returns:
            Dict with keys: result, cost, input_tokens, output_tokens, model, elapsed
        """
        start_time = time.time()

        # Select appropriate model
        model_id = self._select_model_for_complexity(model, complexity_score)
        model_config = GROQ_MODELS.get(model_id)

        # Resolve settings from model defaults + task_type + explicit overrides
        resolved = _resolve_model_settings(
            model_id, task_type=task_type,
            temperature=temperature, top_p=top_p, max_tokens=max_tokens,
        )
        temperature = resolved["temperature"]
        top_p = resolved.get("top_p")
        max_tokens = resolved.get("max_tokens") or 4096

        if not model_config:
            return {
                "error": f"Unknown model: {model_id}",
                "returncode": 1,
                "result": "",
                "cost": 0.0
            }

        # Enforce rate limiting
        self._rate_limit_wait()

        try:
            system_message = (system_prompt or kwargs.get("system_prompt") or DEFAULT_SYSTEM_PROMPT).strip()
            if not system_message:
                system_message = DEFAULT_SYSTEM_PROMPT
            # Make API request
            create_kwargs = {
                "model": model_id,
                "messages": [
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": prompt},
                ],
                "max_tokens": min(max_tokens, model_config.max_completion),
                "temperature": temperature,
                "timeout": timeout,
            }
            if top_p is not None:
                create_kwargs["top_p"] = top_p
            if seed is not None:
                create_kwargs["seed"] = seed
            response = self.client.chat.completions.create(**create_kwargs)

            # Extract response
            result_text = response.choices[0].message.content or ""
            input_tokens = response.usage.prompt_tokens if response.usage else 0
            output_tokens = response.usage.completion_tokens if response.usage else 0

            # Calculate cost via shared llm_cost module
            from vivarium.utils.llm_cost import estimate_cost

            cost = estimate_cost(model_id, input_tokens, output_tokens)

            # Update totals
            self.total_cost += cost
            self.total_input_tokens += input_tokens
            self.total_output_tokens += output_tokens
            self.request_count += 1

            elapsed = time.time() - start_time

            return {
                "result": result_text,
                "returncode": 0,
                "cost": cost,
                "total_cost_usd": cost,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "model": model_id,
                "model_display": model_config.display_name,
                "elapsed": elapsed,
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            elapsed = time.time() - start_time
            error_msg = str(e)

            # Check for specific error types
            if "rate_limit" in error_msg.lower():
                # Try fallback model if we hit rate limit on primary
                if model_id == "openai/gpt-oss-120b":
                    print(f"[GROQ] Rate limit on 120B, falling back to 70B...")
                    time.sleep(2)  # Brief wait before fallback
                    return self.execute(
                        prompt=prompt,
                        model="llama-3.3-70b-versatile",  # Fallback
                        complexity_score=complexity_score,
                        max_tokens=max_tokens,
                        temperature=temperature,
                        timeout=timeout,
                        top_p=top_p,
                        seed=seed,
                        task_type=task_type,
                        system_prompt=system_message,
                    )
                return {
                    "error": "rate_limit",
                    "error_message": error_msg,
                    "returncode": 429,
                    "result": "",
                    "cost": 0.0,
                    "elapsed": elapsed
                }
            elif "timeout" in error_msg.lower():
                return {
                    "error": "timeout",
                    "error_message": error_msg,
                    "returncode": 408,
                    "result": "",
                    "cost": 0.0,
                    "elapsed": elapsed
                }
            else:
                return {
                    "error": "api_error",
                    "error_message": error_msg,
                    "returncode": 1,
                    "result": "",
                    "cost": 0.0,
                    "elapsed": elapsed
                }

    def get_stats(self) -> Dict[str, Any]:
        """Get cumulative usage statistics."""
        return {
            "total_cost_usd": self.total_cost,
            "total_input_tokens": self.total_input_tokens,
            "total_output_tokens": self.total_output_tokens,
            "total_requests": self.request_count,
            "avg_cost_per_request": self.total_cost / max(self.request_count, 1)
        }

    def check_budget(self, budget_limit: float) -> Tuple[bool, float]:
        """
        Check if we're within budget.

        Args:
            budget_limit: Maximum budget in USD

        Returns:
            (within_budget, remaining)
        """
        remaining = budget_limit - self.total_cost
        return remaining > 0, remaining


# Global instance (lazy initialization)
_groq_engine: Optional[GroqInferenceEngine] = None


def get_groq_engine() -> GroqInferenceEngine:
    """Get or create the global Groq engine instance."""
    global _groq_engine
    if _groq_engine is None:
        _groq_engine = GroqInferenceEngine()
    return _groq_engine


def execute_with_groq(
    prompt: str,
    model: str = "openai/gpt-oss-120b",  # DEFAULT: GPT-OSS 120B
    complexity_score: float = 0.0,  # Ignored - Groq Compound handles this
    **kwargs
) -> Dict[str, Any]:
    """
    Convenience function to execute a prompt with Groq.

    Args:
        prompt: The prompt to execute
        model: Model name or alias
        complexity_score: Task complexity for adaptive selection
        **kwargs: Additional arguments passed to execute()

    Returns:
        Execution result dict
    """
    engine = get_groq_engine()
    return engine.execute(prompt, model, complexity_score, **kwargs)


if __name__ == "__main__":
    # Quick test
    print("Testing Groq client...")

    if not os.environ.get("GROQ_API_KEY"):
        print("Set GROQ_API_KEY environment variable to test")
    else:
        engine = GroqInferenceEngine()

        # Test with simple prompt
        result = engine.execute(
            prompt="Say 'Hello from Groq!' and nothing else.",
            model="fast",
            max_tokens=50
        )

        print(f"Result: {result.get('result', 'ERROR')}")
        print(f"Model: {result.get('model')}")
        print(f"Cost: ${result.get('cost', 0):.6f}")
        print(f"Tokens: {result.get('input_tokens', 0)} in / {result.get('output_tokens', 0)} out")
        print(f"Stats: {engine.get_stats()}")