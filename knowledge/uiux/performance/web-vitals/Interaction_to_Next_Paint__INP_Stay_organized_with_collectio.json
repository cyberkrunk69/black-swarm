{
  "id": "web-vitals__articles_inp",
  "source_id": "web-vitals",
  "source_name": "Web Vitals",
  "category": "performance",
  "url": "https://web.dev/articles/inp",
  "title": "Interaction to Next Paint (INP)Stay organized with collectionsSave and categorize content based on your preferences.",
  "content": "Home\nArticles\nInteraction to Next Paint (INP)\nStay organized with collections\nSave and categorize content based on your preferences.\nJeremy Wagner\nBarry Pollard\nPublished: May 06, 2022, Last updated: September 2, 2025\nChrome usage data shows that 90% of a user's time on a page is spent\nafter\nit loads, Thus, careful measurement of responsiveness\nthroughout\nthe page lifecycle is important. This is what the INP metric assesses.\nGood responsiveness means that a page responds quickly to interactions. When a page responds to an interaction, the browser presents\nvisual feedback\nin the next frame that it paints. Visual feedback tells you if, for example, an item you add to an online shopping cart is indeed being added, whether a mobile navigation menu has opened, if a login form's contents are being authenticated by the server, and so forth.\nSome interactions naturally take longer than others, but for especially complex interactions, it's important to quickly present some initial visual feedback to tell the user that something is happening. The next frame that the browser will paint is the earliest opportunity to do this.\nTherefore, the intent of INP is not to measure\nall\nthe eventual effects of an interaction\u2014such as network fetches and UI updates from other asynchronous operations)\u2014but the time that the\nnext\npaint is being blocked. By delaying visual feedback, users may get the impression that the page is not responding quickly enough, and INP was developed to help developers measure this part of the user experience.\nIn the following video, the example on the right gives immediate visual feedback that an accordion is opening. Poor responsiveness is demonstrated in example on the left, and how it can create poor user experiences.\nAn example of poor versus good responsiveness. On the left, long tasks block the accordion from opening. This causes the user to click multiple times, thinking the experience is broken. When the main thread catches up, it processes the delayed inputs, resulting in the accordion opening and closing unexpectedly. On the right, a more responsive page opens the accordion quickly and without incident.\nThis guide explains how INP works, how to measure it, and points to resources for improving it.\nWhat is INP?\nINP is a metric that assesses a page's overall responsiveness to user interactions by observing the latency of all click, tap, and keyboard interactions that occur throughout the lifespan of a user's visit to a page. The final INP value is the longest interaction observed, ignoring outliers.\nDetails on how INP is calculated\nINP is calculated by observing all the interactions made with a page. For most sites the interaction with the worst latency is reported as INP.\nHowever, for pages with large numbers of interactions, random hiccups can result in an unusually high-latency interaction on an otherwise responsive page. The more interactions that occur on a given page, the more likely this is to happen.\nTo give a better measure of the actual responsiveness for pages with a high number of interactions, we ignore one highest interaction for every 50 interactions. The vast majority of page experiences don't have over 50 interactions, so the worst interaction is most often reported. The 75th percentile of all page views is then reported as usual, which further removes outliers to give a value that the vast majority of users experience or better.\nAn\ninteraction\nis a group of event handlers that fire during the same logical user gesture. For example, \"tap\" interactions on a touchscreen device include multiple events, such as\npointerup\n,\npointerdown\n, and\nclick\n. An interaction can be driven by JavaScript, CSS, built-in browser controls (such as form elements), or a combination thereof.\nAn interaction's latency consists of the single longest\nduration\nof a group of event handlers that drive the interaction, from the time the user begins the interaction to the moment the browser is next able to paint a frame. In rare cases there may be no frame to paint but the interaction ends when the browser would be able to paint a frame.\nWhat is a good INP score?\nPinning labels such as \"good\" or \"poor\" on a responsiveness metric is difficult. On one hand, you want to encourage development practices that prioritize good responsiveness. On the other hand, you must account for the fact that there's considerable variability in the capabilities of devices people use to set achievable development expectations.\nTo ensure you're delivering user experiences with good responsiveness, a good threshold to measure is the\n75th percentile\nof page loads recorded in the field, segmented across mobile and desktop devices:\nAn INP below or at\n200 milliseconds\nmeans a page has\ngood responsiveness\n.\nAn INP above\n200 milliseconds\nand below or at\n500 milliseconds\nmeans a page's responsiveness\nneeds improvement\n.\nAn INP above\n500 milliseconds\nmeans a page has\npoor responsiveness\n.\nGood INP values are 200 milliseconds or less. Poor values are greater than 500 milliseconds.\nWhat's in an interaction?\nThe life of an interaction. An input delay occurs until event handlers start running, possibly caused by factors such as long tasks on the main thread. The interaction's event handler callbacks then run, and a delay occurs before the next frame is presented.\nThe primary driver of interactivity is often JavaScript, though browsers do provide interactivity through controls\nnot\npowered by JavaScript, such as checkboxes, radio buttons, and controls powered by CSS.\nAs the purposes of INP,\nonly the following interaction types are observed:\nClicking with a mouse.\nTapping on a device with a touchscreen.\nPressing a key on either a physical or onscreen keyboard.\nInteractions happen in the main document or in iframes embedded in the document\u2014for example clicking play on an embedded video. End users won't be aware what is in an iframe or not, therefore, INP within iframes are needed to measure the user experience for the top level page. Because JavaScript Web APIs don't have access to the contents of iframes, this may\nshow as a difference between CrUX and RUM\nInteractions can consist of multiple events. For example, a keystroke includes the\nkeydown\n,\nkeypress\n, and\nkeyup\nevents. Tap interactions contain\npointerup\nand\npointerdown\nevents. The event with the longest duration within the interaction is what contributes to the interaction's total latency.\nA depiction of an interaction with multiple event handlers. The first part of the interaction receives an input when the user clicks down on a mouse button. However, before they release the mouse button, a frame is presented. When the user releases the mouse button, another series of event handlers must run before the next frame is presented.\nAs shown in the diagram, the\nprocessing duration\nof INP includes all event handler callbacks within that frame. This makes the\ninput delay\nthe time before any callback for an interaction is handled, the\nprocessing duration\nthe time for all the callbacks to execute, and the\npresentation delay\nthe time after the callbacks have been executed until the frame is presented on the user's screen.\nThe page's INP is calculated when the user leaves the page. The result is a single value that is representative of the page's overall responsiveness throughout its lifecycle.\nA low INP means that a page was reliably responsive to user input.\nHow is INP different from First Input Delay (FID)?\nINP is the successor metric to\nFirst Input Delay (FID)\n. While both are responsiveness metrics, FID only measured the\ninput delay\nof the\nfirst\ninteraction on a page. INP improves on FID by observing\nall\ninteractions on a page, beginning from the input delay, to the time it takes to run event handlers, and finally up until the browser has painted the next frame.\nThese differences mean that both INP and FID are different types of responsiveness metrics. Where FID was a\nload responsiveness metric\ndesigned to assess the page's first impression on the user, INP is a more reliable indicator of overall responsiveness, regardless of when in the life of a page interactions occur.\nWhat if no INP value is reported?\nIt's possible for a page to return no INP value. This can happen for a number of reasons, including the following:\nThe page was loaded, but the user never clicked, tapped, or pressed a key on their keyboard.\nThe page loaded, but the user interacted with it using gestures that aren't measured, such as scrolling or hovering over elements.\nThe page is being accessed by a bot such as a search crawler or headless browser that has not been scripted to interact with the page.\nHow to measure INP\nINP can be measured both in\nthe field\nand in\nthe lab\n, to the extent that you can simulate realistic user interactions.\nIn the field\nIdeally, your journey in optimizing INP will start with field data. At its best, field data from Real User Monitoring (RUM) will give you not only a page's INP value, but also contextual data that highlights what specific interaction was responsible for the INP value itself, whether the interaction occurred during or after page load, the type of interaction (click, keypress, or tap), and other valuable timings that can help you identify which part of the interaction was affecting responsiveness.\nIf your website qualifies for inclusion in the\nChrome User Experience Report (CrUX)\n, you can quickly get field data for INP\nvia CrUX in PageSpeed Insights\n(and other Core Web Vitals). At a minimum, you can get an origin-level picture of your website's INP, but in some cases, you can also get URL-level data.\nHowever, while CrUX can tell you if there\nis\na problem, it can't tell you what caused the problem. A RUM solution can help you uncover more details about pages, users, or user interactions that are experiencing responsiveness issues. Being able to attribute INP to individual interactions avoids guesswork and wasted effort.\nIn the lab\nOptimally, you'll want to start testing in the lab once you have field data that suggests a page has slow interactions. Field data will make the work of reproducing problematic interactions in the lab a much more straightforward task.\nIt's entirely possible, however, that you don't have field data. While INP\ncan\nbe measured in some lab tools, the resulting INP value for a page during lab testing will be dependent on what interactions are performed during the measurement period. User behaviors can be unpredictable and highly variable, meaning that your testing in the lab may not surface problem interactions in the same fashion that field data can. Additionally, some lab tools won't report a page's INP because they only observe the loading of a page without any interactions. In such cases,\nTotal Blocking Time (TBT)\nmay be a reasonable proxy metric for INP, but it's not a substitute for INP in and of itself.\nEven though there are limitations in lab tools when it comes to assessing a page's INP, there are some strategies for reproducing slow interactions in the lab. Strategies include following common user flows and testing interactions along the way, as well as interacting with the page as it loads\u2014when the main thread is often busiest\u2014in order to identify slow interactions during that crucial part of the user experience.\nMeasure INP in JavaScript\nTo measure INP in JavaScript, you need to measure event timings for all\ninteractions, and then take the 98th percentile across all these interactions on page unload. You can refer to the\nweb vitals\nJavaScript library source code\nwhich contains a reference implementation on how INP is calculated.\nIn most cases, the current INP value at the time the page is being unloaded is the final INP value for that page, but there are a few important exceptions as noted in the next section. The\nweb vitals\nJavaScript library accounts for these as much as possible, within the limitations of the Web APIs.\nDifferences between the metric and the API\nevent\nentries below 104 milliseconds don't report by default using performance observers. This default can be changed when a performance observer is registered using the\ndurationThreshold\nparameter but even this has a minimum value of 16 milliseconds. For this reason, it is recommended to also observe the\nfirst-input\nentry, which is also an Event Timing entry, but is guaranteed to be observable even when its duration is less than\ndurationThreshold\n. This helps ensure that pages with interactions always report some INP value.\nCalculating percentiles perfectly technically requires keeping all samples in memory, which can be costly. But you can approximate percentiles, especially really high percentiles like p98, by just keeping a small list of the worst-N interactions. 10 is a common choice.\nIf a page is restored from the\nback/forward cache\n, its INP value should be reset to zero since users experience this as a distinct page visit.\nThe API does not report\nevent\nentries for interactions that occur within iframes but the metric does as they are part of the user experience of the page. This can\nshow as a difference between CrUX and RUM\n. To properly measure INP you should consider them. Sub-frames can use the API to report their\nevent-timing\nentries to the parent frame.\nIn addition to these exceptions, INP has some added complexity due to the fact that it measures the entire lifespan of a page:\nUsers might keep a tab open for a\nvery\nlong time\u2014days, weeks, months. In fact, a user might never close a tab.\nOn mobile operating systems, browsers typically don't run page unload callbacks for background tabs, making it difficult to report the \"final\" value.\nTo handle such cases, INP should be reported any time a page is background\u2014in addition to any time it's unloaded (the\nvisibilitychange\nevent\ncovers both of these scenarios). And analytics systems receiving this data will then need to calculate the final INP value on the backend.\nRather than memorizing and grappling with all of these cases yourself, developers can use the\nweb-vitals\nJavaScript library\nto measure INP, which accounts for everything mentioned previously, except the iframe case:\nimport\n{\nonINP\n}\nfrom\n'web-vitals'\n;\n// Measure and log INP in all situations\n// where it needs to be reported.\nonINP\n(\nconsole\n.\nlog\n);\nHow to improve INP\nA\ncollection of guides on optimizing INP\nis available to guide you through the process of identifying slow interactions in the field, and using lab data to help you identify causes and optimize them.\nChangelog\nOccasionally, bugs are discovered in the APIs used to measure metrics, and sometimes in the definitions of the metrics themselves. As a result, changes must sometimes be made, and these changes can show up as improvements or regressions in your internal reports and dashboards.\nTo help you manage this, all changes to either the implementation or definition of these metrics will be surfaced in this\nChangelog\n.\nIf you have feedback for these metrics, provide it in the\nweb-vitals-feedback Google group\n.\nTest your knowledge\nWhat is the primary goal of the INP metric?\nTo measure the time it takes for the first content of a page to be displayed.\nIncorrect - This describes First Contentful Paint\nTo quantify the visual stability of a page and minimize unexpected layout shifts.\nIncorrect - This describes Cumulative Layout Shift\nTo assess the time it takes for a page to become fully interactive.\nIncorrect - This is related to Time to Interactive, but INP specifically focuses on responsiveness to user input\nTo minimize the time from when a user initiates an interaction until the next frame is painted, for all or most interactions the user initiates.\nCorrect!\nWhich of the following interaction types are observed for the purposes of calculating INP? (Select all that apply.)\nClicking with a mouse.\nCorrect!\nScrolling the page with a mouse wheel or trackpad.\nIncorrect - INP does not consider scrolling\nTapping on a touchscreen.\nCorrect!\nHovering the mouse cursor over elements.\nIncorrect - INP does not consider hovering\nPressing a key on a keyboard.\nCorrect!\nZooming in or out on the page.\nIncorrect - INP does not consider zooming\nHow is the \"latency\" of an interaction defined for INP?\nThe amount of time for the browser to process the event handlers of an interaction.\nIncorrect - This only accounts for the processing duration, not the input delay or time to present the next frame\nThe average time it takes for all interactions on a page to produce a visual response.\nIncorrect - INP focuses on the longest interaction, not the average\nThe time it takes for the browser to start processing the event handlers associated with an interaction.\nIncorrect - This only accounts for the input delay, not the processing and rendering time\nThe time from the start of the interaction to the moment the next frame is fully presented.\nCorrect!\nWhat is the difference between INP and FID?\nINP measures the time it takes for the first content of a page to be displayed, while FID measures the responsiveness to user input.\nIncorrect - This describes First Contentful Paint, not INP\nINP considers the full duration of all interactions, while FID only measures the input delay of the first interaction.\nCorrect!\nINP and FID measure different timestamps at which a page becomes interactive.\nIncorrect - INP and FID are measures of how quickly the page responds to interactions, irrespective of when the interactions occur\nThere is no difference; INP and FID are just two different names for the same metric.\nIncorrect - They do have distinct definitions\nUnder what circumstances might INP data be unavailable for a page in tools like PageSpeed Insights?\nThe page is using a custom performance measurement library that doesn't report INP data.\nIncorrect - INP is automatically measured using web platform APIs and doesn't rely on pages self-reporting their performance through custom libraries.\nThere isn't enough interaction data from Chrome users to calculate a meaningful INP value in the CrUX dataset.\nCorrect!\nUsers interacted with the page solely through scrolling and hovering, which are not considered for INP.\nCorrect!\nThe page is built using a framework that automatically optimizes for INP, so there's no need to report it.\nIncorrect - Frameworks can help with INP, but the metric is still relevant and reported if data is available\nWhat is the\nmost effective\nstrategy for reproducing slow interactions in a lab environment?\nSimulating a high-end device with a slow and unreliable network connection to create challenging conditions.\nIncorrect - While network can play a role, device capabilities are more likely to expose slow interactions\nTesting interactions only after the page has fully loaded and is idle.\nIncorrect - This might miss interactions that are slow during load\nInteracting with the page during load and following common user flows to identify potential bottlenecks.\nCorrect!\nFocusing on complex, edge-case interactions that are unlikely to be encountered by most users.\nIncorrect - Common user flows are more relevant for identifying typical INP issues\n\u2728\nThis quiz was generated by Gemini 1.5 and reviewed by humans.\nShare your feedback\nExcept as otherwise noted, the content of this page is licensed under the\nCreative Commons Attribution 4.0 License\n, and code samples are licensed under the\nApache 2.0 License\n. For details, see the\nGoogle Developers Site Policies\n. Java is a registered trademark of Oracle and/or its affiliates.\nLast updated 2025-09-02 UTC.",
  "content_markdown": "- [Home](https://web.dev/)\n- [Articles](https://web.dev/articles)\n\n# Interaction to Next Paint (INP) Stay organized with collections Save and categorize content based on your preferences.\n\n![Jeremy Wagner](https://web.dev/images/authors/jlwagner-v6.jpg)\n\nJeremy Wagner\n\n![Barry Pollard](https://web.dev/images/authors/tunetheweb.jpg)\n\nBarry Pollard\n\nPublished: May 06, 2022, Last updated: September 2, 2025\n\nChrome usage data shows that 90% of a user's time on a page is spent *after* it loads, Thus, careful measurement of responsiveness *throughout* the page lifecycle is important. This is what the INP metric assesses.\n\nGood responsiveness means that a page responds quickly to interactions. When a page responds to an interaction, the browser presents *visual feedback* in the next frame that it paints. Visual feedback tells you if, for example, an item you add to an online shopping cart is indeed being added, whether a mobile navigation menu has opened, if a login form's contents are being authenticated by the server, and so forth.\n\nSome interactions naturally take longer than others, but for especially complex interactions, it's important to quickly present some initial visual feedback to tell the user that something is happening. The next frame that the browser will paint is the earliest opportunity to do this.\n\nTherefore, the intent of INP is not to measure *all* the eventual effects of an interaction\u2014such as network fetches and UI updates from other asynchronous operations)\u2014but the time that the *next* paint is being blocked. By delaying visual feedback, users may get the impression that the page is not responding quickly enough, and INP was developed to help developers measure this part of the user experience.\n\nIn the following video, the example on the right gives immediate visual feedback that an accordion is opening. Poor responsiveness is demonstrated in example on the left, and how it can create poor user experiences.\n\n[\n\n](/static/articles/inp/video/jL3OLOhcWUQDnR4XjewLBx4e3PC3/WSmcjiQC4lyLxGoES4dd.mp4)\n\nAn example of poor versus good responsiveness. On the left, long tasks block the accordion from opening. This causes the user to click multiple times, thinking the experience is broken. When the main thread catches up, it processes the delayed inputs, resulting in the accordion opening and closing unexpectedly. On the right, a more responsive page opens the accordion quickly and without incident.\n\nThis guide explains how INP works, how to measure it, and points to resources for improving it.\n\n## What is INP?\n\nINP is a metric that assesses a page's overall responsiveness to user interactions by observing the latency of all click, tap, and keyboard interactions that occur throughout the lifespan of a user's visit to a page. The final INP value is the longest interaction observed, ignoring outliers.\n\nDetails on how INP is calculated\n\nINP is calculated by observing all the interactions made with a page. For most sites the interaction with the worst latency is reported as INP.\n\nHowever, for pages with large numbers of interactions, random hiccups can result in an unusually high-latency interaction on an otherwise responsive page. The more interactions that occur on a given page, the more likely this is to happen.\n\nTo give a better measure of the actual responsiveness for pages with a high number of interactions, we ignore one highest interaction for every 50 interactions. The vast majority of page experiences don't have over 50 interactions, so the worst interaction is most often reported. The 75th percentile of all page views is then reported as usual, which further removes outliers to give a value that the vast majority of users experience or better.\n\nAn *interaction* is a group of event handlers that fire during the same logical user gesture. For example, \"tap\" interactions on a touchscreen device include multiple events, such as `pointerup`, `pointerdown`, and `click`. An interaction can be driven by JavaScript, CSS, built-in browser controls (such as form elements), or a combination thereof.\n\nAn interaction's latency consists of the single longest [duration](https://w3c.github.io/event-timing/#ref-for-dom-performanceentry-duration%E2%91%A1:%7E:text=The%20Event%20Timing%20API%20exposes%20a%20duration%20value%2C%20which%20is%20meant%20to%20be%20the%20time%20from%20when%20user%20interaction%20occurs%20(estimated%20via%20the%20Event%27s%20timeStamp)%20to%20the%20next%20time%20the%20rendering%20of%20the%20Event%27s%20relevant%20global%20object%27s%20associated%20Document%E2%80%99s%20is%20updated) of a group of event handlers that drive the interaction, from the time the user begins the interaction to the moment the browser is next able to paint a frame. In rare cases there may be no frame to paint but the interaction ends when the browser would be able to paint a frame.\n\n### What is a good INP score?\n\nPinning labels such as \"good\" or \"poor\" on a responsiveness metric is difficult. On one hand, you want to encourage development practices that prioritize good responsiveness. On the other hand, you must account for the fact that there's considerable variability in the capabilities of devices people use to set achievable development expectations.\n\nTo ensure you're delivering user experiences with good responsiveness, a good threshold to measure is the **75th percentile** of page loads recorded in the field, segmented across mobile and desktop devices:\n\n- An INP below or at **200 milliseconds** means a page has **good responsiveness**.\n- An INP above **200 milliseconds** and below or at **500 milliseconds** means a page's responsiveness **needs improvement**.\n- An INP above **500 milliseconds** means a page has **poor responsiveness**.\n\n![Good INP values are 200 milliseconds or less, poor values are greater than 500 milliseconds, and anything in between needs improvement.](/static/articles/inp/image/inp-mobile-v2.svg)\n\nGood INP values are 200 milliseconds or less. Poor values are greater than 500 milliseconds.\n\n### What's in an interaction?\n\n![A diagram depicting an interaction on the main thread. The user makes an input while blocking tasks run. The input is delayed until those tasks complete, after which the pointerup, mouseup, and click event handlers run, then rendering and painting work is kicked off until the next frame is presented.](/static/articles/inp/image/whats-in-an-interaction.svg)\n\nThe life of an interaction. An input delay occurs until event handlers start running, possibly caused by factors such as long tasks on the main thread. The interaction's event handler callbacks then run, and a delay occurs before the next frame is presented.\n\nThe primary driver of interactivity is often JavaScript, though browsers do provide interactivity through controls *not* powered by JavaScript, such as checkboxes, radio buttons, and controls powered by CSS.\n\nAs the purposes of INP, **only the following interaction types are observed:**\n\n- Clicking with a mouse.\n- Tapping on a device with a touchscreen.\n- Pressing a key on either a physical or onscreen keyboard.\n\nInteractions happen in the main document or in iframes embedded in the document\u2014for example clicking play on an embedded video. End users won't be aware what is in an iframe or not, therefore, INP within iframes are needed to measure the user experience for the top level page. Because JavaScript Web APIs don't have access to the contents of iframes, this may [show as a difference between CrUX and RUM](/articles/crux-and-rum-differences#iframes)\n\nInteractions can consist of multiple events. For example, a keystroke includes the `keydown`, `keypress`, and `keyup` events. Tap interactions contain `pointerup` and `pointerdown` events. The event with the longest duration within the interaction is what contributes to the interaction's total latency.\n\n![A depiction of more complex interaction containing two interactions. The first is a mousedown event, which produces a frame before the mouse button is let up, which kicks off more work until yet another frame is presented as the result.](/static/articles/inp/image/logical-interaction.svg)\n\nA depiction of an interaction with multiple event handlers. The first part of the interaction receives an input when the user clicks down on a mouse button. However, before they release the mouse button, a frame is presented. When the user releases the mouse button, another series of event handlers must run before the next frame is presented.\n\nAs shown in the diagram, the **processing duration** of INP includes all event handler callbacks within that frame. This makes the **input delay** the time before any callback for an interaction is handled, the **processing duration** the time for all the callbacks to execute, and the **presentation delay** the time after the callbacks have been executed until the frame is presented on the user's screen.\n\nThe page's INP is calculated when the user leaves the page. The result is a single value that is representative of the page's overall responsiveness throughout its lifecycle. **A low INP means that a page was reliably responsive to user input.**\n\n### How is INP different from First Input Delay (FID)?\n\nINP is the successor metric to [First Input Delay (FID)](/articles/fid). While both are responsiveness metrics, FID only measured the [input delay](/articles/optimize-input-delay#what_is_input_delay) of the *first* interaction on a page. INP improves on FID by observing *all* interactions on a page, beginning from the input delay, to the time it takes to run event handlers, and finally up until the browser has painted the next frame.\n\nThese differences mean that both INP and FID are different types of responsiveness metrics. Where FID was a [load responsiveness metric](/articles/user-centric-performance-metrics#types_of_metrics) designed to assess the page's first impression on the user, INP is a more reliable indicator of overall responsiveness, regardless of when in the life of a page interactions occur.\n\n### What if no INP value is reported?\n\nIt's possible for a page to return no INP value. This can happen for a number of reasons, including the following:\n\n- The page was loaded, but the user never clicked, tapped, or pressed a key on their keyboard.\n- The page loaded, but the user interacted with it using gestures that aren't measured, such as scrolling or hovering over elements.\n- The page is being accessed by a bot such as a search crawler or headless browser that has not been scripted to interact with the page.\n\n## How to measure INP\n\nINP can be measured both in [the field](/articles/lab-and-field-data-differences#field_data) and in [the lab](/articles/lab-and-field-data-differences#lab_data), to the extent that you can simulate realistic user interactions.\n\n### In the field\n\nIdeally, your journey in optimizing INP will start with field data. At its best, field data from Real User Monitoring (RUM) will give you not only a page's INP value, but also contextual data that highlights what specific interaction was responsible for the INP value itself, whether the interaction occurred during or after page load, the type of interaction (click, keypress, or tap), and other valuable timings that can help you identify which part of the interaction was affecting responsiveness.\n\nIf your website qualifies for inclusion in the [Chrome User Experience Report (CrUX)](https://developer.chrome.com/docs/crux), you can quickly get field data for INP [via CrUX in PageSpeed Insights](/articles/find-slow-interactions-in-the-field#get_field_data_quickly_with_crux) (and other Core Web Vitals). At a minimum, you can get an origin-level picture of your website's INP, but in some cases, you can also get URL-level data.\n\nHowever, while CrUX can tell you if there *is* a problem, it can't tell you what caused the problem. A RUM solution can help you uncover more details about pages, users, or user interactions that are experiencing responsiveness issues. Being able to attribute INP to individual interactions avoids guesswork and wasted effort.\n\n### In the lab\n\nOptimally, you'll want to start testing in the lab once you have field data that suggests a page has slow interactions. Field data will make the work of reproducing problematic interactions in the lab a much more straightforward task.\n\nIt's entirely possible, however, that you don't have field data. While INP *can* be measured in some lab tools, the resulting INP value for a page during lab testing will be dependent on what interactions are performed during the measurement period. User behaviors can be unpredictable and highly variable, meaning that your testing in the lab may not surface problem interactions in the same fashion that field data can. Additionally, some lab tools won't report a page's INP because they only observe the loading of a page without any interactions. In such cases, [Total Blocking Time (TBT)](/articles/tbt) may be a reasonable proxy metric for INP, but it's not a substitute for INP in and of itself.\n\nEven though there are limitations in lab tools when it comes to assessing a page's INP, there are some strategies for reproducing slow interactions in the lab. Strategies include following common user flows and testing interactions along the way, as well as interacting with the page as it loads\u2014when the main thread is often busiest\u2014in order to identify slow interactions during that crucial part of the user experience.\n\n### Measure INP in JavaScript\n\nTo measure INP in JavaScript, you need to measure event timings for all\ninteractions, and then take the 98th percentile across all these interactions on page unload. You can refer to the [`web vitals` JavaScript library source code](https://github.com/GoogleChrome/web-vitals/blob/main/src/onINP.ts) which contains a reference implementation on how INP is calculated.\n\nIn most cases, the current INP value at the time the page is being unloaded is the final INP value for that page, but there are a few important exceptions as noted in the next section. The `web vitals` JavaScript library accounts for these as much as possible, within the limitations of the Web APIs.\n\n#### Differences between the metric and the API\n\n- `event` entries below 104 milliseconds don't report by default using performance observers. This default can be changed when a performance observer is registered using the `durationThreshold` parameter but even this has a minimum value of 16 milliseconds. For this reason, it is recommended to also observe the `first-input` entry, which is also an Event Timing entry, but is guaranteed to be observable even when its duration is less than `durationThreshold`. This helps ensure that pages with interactions always report some INP value.\n- Calculating percentiles perfectly technically requires keeping all samples in memory, which can be costly. But you can approximate percentiles, especially really high percentiles like p98, by just keeping a small list of the worst-N interactions. 10 is a common choice.\n- If a page is restored from the [back/forward cache](/articles/bfcache#impact_on_core_web_vitals), its INP value should be reset to zero since users experience this as a distinct page visit.\n- The API does not report `event` entries for interactions that occur within iframes but the metric does as they are part of the user experience of the page. This can [show as a difference between CrUX and RUM](/articles/crux-and-rum-differences#iframes). To properly measure INP you should consider them. Sub-frames can use the API to report their `event-timing` entries to the parent frame.\n\nIn addition to these exceptions, INP has some added complexity due to the fact that it measures the entire lifespan of a page:\n\n- Users might keep a tab open for a *very* long time\u2014days, weeks, months. In fact, a user might never close a tab.\n- On mobile operating systems, browsers typically don't run page unload callbacks for background tabs, making it difficult to report the \"final\" value.\n\nTo handle such cases, INP should be reported any time a page is background\u2014in addition to any time it's unloaded (the [`visibilitychange` event](https://developer.chrome.com/blog/page-lifecycle-api#event-visibilitychange) covers both of these scenarios). And analytics systems receiving this data will then need to calculate the final INP value on the backend.\n\nRather than memorizing and grappling with all of these cases yourself, developers can use the [`web-vitals` JavaScript library](https://github.com/GoogleChrome/web-vitals) to measure INP, which accounts for everything mentioned previously, except the iframe case:\n\n```\nimport {onINP} from 'web-vitals';\n\n// Measure and log INP in all situations\n// where it needs to be reported.\nonINP(console.log);\n```\n\n## How to improve INP\n\nA [collection of guides on optimizing INP](/articles/optimize-inp) is available to guide you through the process of identifying slow interactions in the field, and using lab data to help you identify causes and optimize them.\n\n## Changelog\n\nOccasionally, bugs are discovered in the APIs used to measure metrics, and sometimes in the definitions of the metrics themselves. As a result, changes must sometimes be made, and these changes can show up as improvements or regressions in your internal reports and dashboards.\n\nTo help you manage this, all changes to either the implementation or definition of these metrics will be surfaced in this [Changelog](https://chromium.googlesource.com/chromium/src/+/main/docs/speed/metrics_changelog/inp.md).\n\nIf you have feedback for these metrics, provide it in the [web-vitals-feedback Google group](https://groups.google.com/g/web-vitals-feedback).\n\n## Test your knowledge\n\nWhat is the primary goal of the INP metric?\n\nTo measure the time it takes for the first content of a page to be displayed.\n\nIncorrect - This describes First Contentful Paint\n\nTo quantify the visual stability of a page and minimize unexpected layout shifts.\n\nIncorrect - This describes Cumulative Layout Shift\n\nTo assess the time it takes for a page to become fully interactive.\n\nIncorrect - This is related to Time to Interactive, but INP specifically focuses on responsiveness to user input\n\nTo minimize the time from when a user initiates an interaction until the next frame is painted, for all or most interactions the user initiates.\n\nCorrect!\n\nWhich of the following interaction types are observed for the purposes of calculating INP? (Select all that apply.)\n\nClicking with a mouse.\n\nCorrect!\n\nScrolling the page with a mouse wheel or trackpad.\n\nIncorrect - INP does not consider scrolling\n\nTapping on a touchscreen.\n\nCorrect!\n\nHovering the mouse cursor over elements.\n\nIncorrect - INP does not consider hovering\n\nPressing a key on a keyboard.\n\nCorrect!\n\nZooming in or out on the page.\n\nIncorrect - INP does not consider zooming\n\nHow is the \"latency\" of an interaction defined for INP?\n\nThe amount of time for the browser to process the event handlers of an interaction.\n\nIncorrect - This only accounts for the processing duration, not the input delay or time to present the next frame\n\nThe average time it takes for all interactions on a page to produce a visual response.\n\nIncorrect - INP focuses on the longest interaction, not the average\n\nThe time it takes for the browser to start processing the event handlers associated with an interaction.\n\nIncorrect - This only accounts for the input delay, not the processing and rendering time\n\nThe time from the start of the interaction to the moment the next frame is fully presented.\n\nCorrect!\n\nWhat is the difference between INP and FID?\n\nINP measures the time it takes for the first content of a page to be displayed, while FID measures the responsiveness to user input.\n\nIncorrect - This describes First Contentful Paint, not INP\n\nINP considers the full duration of all interactions, while FID only measures the input delay of the first interaction.\n\nCorrect!\n\nINP and FID measure different timestamps at which a page becomes interactive.\n\nIncorrect - INP and FID are measures of how quickly the page responds to interactions, irrespective of when the interactions occur\n\nThere is no difference; INP and FID are just two different names for the same metric.\n\nIncorrect - They do have distinct definitions\n\nUnder what circumstances might INP data be unavailable for a page in tools like PageSpeed Insights?\n\nThe page is using a custom performance measurement library that doesn't report INP data.\n\nIncorrect - INP is automatically measured using web platform APIs and doesn't rely on pages self-reporting their performance through custom libraries.\n\nThere isn't enough interaction data from Chrome users to calculate a meaningful INP value in the CrUX dataset.\n\nCorrect!\n\nUsers interacted with the page solely through scrolling and hovering, which are not considered for INP.\n\nCorrect!\n\nThe page is built using a framework that automatically optimizes for INP, so there's no need to report it.\n\nIncorrect - Frameworks can help with INP, but the metric is still relevant and reported if data is available\n\nWhat is the *most effective* strategy for reproducing slow interactions in a lab environment?\n\nSimulating a high-end device with a slow and unreliable network connection to create challenging conditions.\n\nIncorrect - While network can play a role, device capabilities are more likely to expose slow interactions\n\nTesting interactions only after the page has fully loaded and is idle.\n\nIncorrect - This might miss interactions that are slow during load\n\nInteracting with the page during load and following common user flows to identify potential bottlenecks.\n\nCorrect!\n\nFocusing on complex, edge-case interactions that are unlikely to be encountered by most users.\n\nIncorrect - Common user flows are more relevant for identifying typical INP issues\n\n\u2728 *This quiz was generated by Gemini 1.5 and reviewed by humans. [Share your feedback](https://issuetracker.google.com/issues/new?component=1400680&template=1996170)*\n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-09-02 UTC.\n\n[[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-09-02 UTC.\"],[],[]]",
  "tags": [
    "performance",
    "web-vitals",
    "google",
    "optimization"
  ],
  "extracted_at": "2026-02-03T12:49:13.742675+00:00",
  "content_length": 19467,
  "content_hash": "5b8a13fe5737b9f1"
}