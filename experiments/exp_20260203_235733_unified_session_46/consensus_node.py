"""
consensus_node.py

Implements a multi‑model consensus protocol (3‑round) for critical decisions.

Protocol (as described in SWARM_ARCHITECTURE_V2.md)
-------------------------------------------------
1. **Generation** – Three independent model instances generate candidate
   solutions for a given prompt.
2. **Blind Judging** – The candidates are anonymised and each model ranks
   the other two solutions (no self‑ranking).  Rankings are aggregated.
3. **Debate** – If no solution receives a strict majority (>50 % of the
   rankings), the models enter a short debate round where they can
   propose refinements.  After debate a second blind‑judging round is run.
4. **Consensus / Dissent** – If a consensus is reached, the winning solution
   is returned.  Otherwise the node returns the top‑ranked solution together
   with a log of dissenting opinions.

The implementation below is deliberately lightweight – the “models” are
represented by callables that accept a prompt and return a string.  In a real
deployment these callables would wrap LLM APIs (e.g. OpenAI, Anthropic, etc.).
"""

import asyncio
import logging
import uuid
from collections import Counter
from dataclasses import dataclass, field
from typing import Callable, List, Tuple, Dict, Any

# --------------------------------------------------------------------------- #
# Logging configuration
# --------------------------------------------------------------------------- #
logger = logging.getLogger("ConsensusNode")
logger.setLevel(logging.DEBUG)
handler = logging.FileHandler(
    "experiments/exp_20260203_235733_unified_session_46/consensus_node.log"
)
formatter = logging.Formatter(
    "%(asctime)s - %(levelname)s - %(message)s"
)
handler.setFormatter(formatter)
logger.addHandler(handler)


# --------------------------------------------------------------------------- #
# Data structures
# --------------------------------------------------------------------------- #
@dataclass
class Candidate:
    """A candidate solution generated by a model."""
    id: str
    content: str
    source_model: str  # identifier of the model that produced it


@dataclass
class Ranking:
    """Ranking provided by a model for a set of candidates."""
    model: str
    ordered_candidate_ids: List[str]  # highest rank first


# --------------------------------------------------------------------------- #
# Core consensus node
# --------------------------------------------------------------------------- #
class ConsensusNode:
    """
    Orchestrates the 3‑round consensus protocol among a set of model callables.
    """

    def __init__(self, model_callables: List[Tuple[str, Callable[[str], str]]]):
        """
        Parameters
        ----------
        model_callables : List[Tuple[str, Callable[[str], str]]]
            A list of (model_name, model_callable) tuples.
            Each callable receives a prompt (str) and returns a solution (str).
        """
        if len(model_callables) < 3:
            raise ValueError("At least three models are required for consensus.")
        self.models = model_callables
        logger.debug(f"ConsensusNode initialized with models: {[m[0] for m in self.models]}")

    async def _generate_candidate(self, model_name: str, model_fn: Callable[[str], str], prompt: str) -> Candidate:
        """Wrap model call in an async task."""
        logger.debug(f"Generating candidate with model '{model_name}'.")
        # In a real async LLM call, you would await an async HTTP request.
        # Here we simply run the callable synchronously inside a thread pool.
        loop = asyncio.get_running_loop()
        content = await loop.run_in_executor(None, model_fn, prompt)
        candidate_id = str(uuid.uuid4())
        logger.debug(f"Model '{model_name}' produced candidate {candidate_id}.")
        return Candidate(id=candidate_id, content=content, source_model=model_name)

    async def generation_phase(self, prompt: str) -> List[Candidate]:
        """Phase 1 – each model independently generates a solution."""
        tasks = [
            self._generate_candidate(name, fn, prompt)
            for name, fn in self.models
        ]
        candidates = await asyncio.gather(*tasks)
        logger.info(f"Generation phase completed with {len(candidates)} candidates.")
        return candidates

    async def blind_judging_phase(self, candidates: List[Candidate]) -> List[Ranking]:
        """
        Phase 2 – each model ranks the *other* candidates (no self‑ranking).
        Returns a list of Ranking objects.
        """
        # Build a map for quick lookup
        candidate_map = {c.id: c for c in candidates}
        rankings: List[Ranking] = []

        for judge_name, judge_fn in self.models:
            # Prepare the list of candidate contents (anonymised)
            # In a real scenario, we would feed only the content, not the source.
            other_candidates = [
                (c.id, c.content) for c in candidates if c.source_model != judge_name
            ]

            # Simple heuristic ranking: longer content wins (placeholder)
            # Replace with a proper LLM evaluation in production.
            ordered = sorted(other_candidates, key=lambda x: len(x[1]), reverse=True)
            ordered_ids = [cid for cid, _ in ordered]

            rankings.append(Ranking(model=judge_name, ordered_candidate_ids=ordered_ids))
            logger.debug(f"Model '{judge_name}' ranking: {ordered_ids}")

        logger.info("Blind judging phase completed.")
        return rankings

    def _aggregate_rankings(self, rankings: List[Ranking]) -> Tuple[Candidate, bool, List[Tuple[str, str]]]:
        """
        Aggregates rankings to find a consensus.

        Returns
        -------
        winner_candidate : Candidate
            The candidate with the highest aggregated score.
        consensus_reached : bool
            True if the winner has >50 % of the total possible points.
        dissent_log : List[Tuple[str, str]]
            List of (model_name, dissent_reason) for models that did not rank the winner first.
        """
        # Simple Borda count: first place = 2 points, second = 1 point (for 3 candidates)
        score_counter: Counter = Counter()
        for ranking in rankings:
            for position, cand_id in enumerate(ranking.ordered_candidate_ids):
                points = max(0, len(ranking.ordered_candidate_ids) - position)
                score_counter[cand_id] += points

        # Determine winner
        winner_id, winner_score = score_counter.most_common(1)[0]

        # Total possible points = number_of_models * (number_of_candidates - 1) * max_points_per_rank
        max_points_per_rank = len(rankings[0].ordered_candidate_ids)
        total_possible = len(rankings) * max_points_per_rank * max_points_per_rank
        consensus = winner_score > (total_possible / 2)

        # Build dissent log
        dissent_log: List[Tuple[str, str]] = []
        for ranking in rankings:
            top_choice = ranking.ordered_candidate_ids[0] if ranking.ordered_candidate_ids else None
            if top_choice != winner_id:
                dissent_log.append(
                    (ranking.model, f"Preferred candidate {top_choice} over winner {winner_id}")
                )

        logger.debug(f"Aggregated scores: {score_counter}")
        logger.debug(f"Winner: {winner_id} (score={winner_score}), consensus={consensus}")
        return winner_id, consensus, dissent_log

    async def debate_phase(self, candidates: List[Candidate], prompt: str) -> List[Candidate]:
        """
        Phase 3 – simple debate where each model can suggest a refinement
        to the current top candidate.  The refined text replaces the original
        candidate for that model.
        """
        logger.info("Debate phase started.")
        # Identify current top candidate (by simple length heuristic)
        top_candidate = max(candidates, key=lambda c: len(c.content))

        async def refine(candidate: Candidate, model_name: str, model_fn: Callable[[str], str]) -> Candidate:
            # Prompt the model to improve the candidate
            debate_prompt = (
                f"Original solution:\n{candidate.content}\n\n"
                f"Please provide a concise improvement or alternative that addresses any "
                f"potential shortcomings. Keep the response focused and under 200 words."
            )
            refined_content = await self._generate_candidate(model_name, model_fn, debate_prompt)
            # Keep the same ID to preserve anonymity in subsequent judging
            return Candidate(id=candidate.id, content=refined_content.content, source_model=model_name)

        tasks = [
            refine(c, name, fn) for (name, fn), c in zip(self.models, candidates)
        ]
        refined_candidates = await asyncio.gather(*tasks)
        logger.info("Debate phase completed.")
        return refined_candidates

    async def run(self, prompt: str) -> Dict[str, Any]:
        """
        Executes the full 3‑round protocol.

        Returns
        -------
        dict
            {
                "winner": Candidate,
                "consensus": bool,
                "dissent_log": List[Tuple[str, str]],
                "all_candidates": List[Candidate]
            }
        """
        # ---- Generation ----
        candidates = await self.generation_phase(prompt)

        # ---- Blind Judging ----
        rankings = await self.blind_judging_phase(candidates)
        winner_id, consensus, dissent_log = self._aggregate_rankings(rankings)

        if consensus:
            winner = next(c for c in candidates if c.id == winner_id)
            logger.info("Consensus reached after first judging round.")
            return {
                "winner": winner,
                "consensus": True,
                "dissent_log": dissent_log,
                "all_candidates": candidates,
            }

        # ---- Debate ----
        debated_candidates = await self.debate_phase(candidates, prompt)

        # Second judging round after debate
        rankings2 = await self.blind_judging_phase(debated_candidates)
        winner_id2, consensus2, dissent_log2 = self._aggregate_rankings(rankings2)

        winner = next(c for c in debated_candidates if c.id == winner_id2)
        logger.info(
            f"Debate concluded. Consensus={'yes' if consensus2 else 'no'}."
        )
        return {
            "winner": winner,
            "consensus": consensus2,
            "dissent_log": dissent_log if not consensus else dissent_log2,
            "all_candidates": debated_candidates,
        }


# --------------------------------------------------------------------------- #
# Example usage (can be removed or guarded by __main__)
# --------------------------------------------------------------------------- #
if __name__ == "__main__":
    # Dummy model callables for illustration.
    def model_a(prompt: str) -> str:
        return f"Model A solution to '{prompt[:30]}...'"

    def model_b(prompt: str) -> str:
        return f"Model B proposes a different approach for '{prompt[:30]}...'"

    def model_c(prompt: str) -> str:
        return f"Model C's answer: consider edge cases of '{prompt[:30]}...'"

    node = ConsensusNode([
        ("model_a", model_a),
        ("model_b", model_b),
        ("model_c", model_c),
    ])

    test_prompt = "How should a distributed system handle node failures while maintaining consistency?"
    result = asyncio.run(node.run(test_prompt))

    print("=== Consensus Result ===")
    print(f"Consensus reached: {result['consensus']}")
    print(f"Winner (from {result['winner'].source_model}):\n{result['winner'].content}")
    if result['dissent_log']:
        print("\nDissenting opinions:")
        for model, reason in result['dissent_log']:
            print(f"- {model}: {reason}")