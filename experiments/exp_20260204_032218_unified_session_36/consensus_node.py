"""
consensus_node.py

Implements the multi‚Äëmodel debate protocol described in
SWARM_ARCHITECTURE_V2.md.

The protocol consists of three rounds:

1. **Generation** ‚Äì three independent models generate candidate solutions.
2. **Blind Judging** ‚Äì the candidates are anonymised and each judge (model)
   ranks them without seeing the source.
3. **Debate** ‚Äì if no consensus is reached after judging, the models enter a
   short debate to try to converge on a single answer.

The node logs all dissenting opinions and returns the final consensus
solution (or ``None`` if consensus could not be achieved).
"""

from __future__ import annotations

import logging
from collections import Counter, defaultdict
from dataclasses import dataclass, field
from typing import Callable, List, Tuple, Dict, Any

# --------------------------------------------------------------------------- #
# Logging configuration ‚Äì the host application can reconfigure if needed.
# --------------------------------------------------------------------------- #
logger = logging.getLogger(__name__)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter(
        "[%(asctime)s] %(levelname)s %(name)s ‚Äì %(message)s"
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)


# --------------------------------------------------------------------------- #
# Data structures
# --------------------------------------------------------------------------- #
@dataclass
class Candidate:
    """A solution generated by a model."""
    content: str
    source_id: str  # identifier of the generating model (e.g., "model_A")
    anon_id: str = field(default_factory=str)  # filled during blind phase


@dataclass
class Judgment:
    """Ranking given by a judge model."""
    judge_id: str
    ranking: List[str]  # list of anon_id's ordered from best to worst


# --------------------------------------------------------------------------- #
# Core consensus logic
# --------------------------------------------------------------------------- #
class ConsensusNode:
    """
    Orchestrates the 3‚Äëround consensus protocol.

    Parameters
    ----------
    generator_models : List[Callable[[str], str]]
        Callable objects that accept a prompt and return a solution string.
        Exactly three generators are expected.
    judge_models : List[Callable[[List[str]], List[int]]]
        Callable objects that accept a list of anonymised solution strings and
        return a ranking expressed as a list of indices (0‚Äëbased) where the
        first element is the index of the best solution.
        Exactly three judges are expected.
    max_debate_rounds : int, optional
        Upper bound for the debate phase. Default is 2.
    """

    def __init__(
        self,
        generator_models: List[Callable[[str], str]],
        judge_models: List[Callable[[List[str]], List[int]]],
        max_debate_rounds: int = 2,
    ):
        if len(generator_models) != 3:
            raise ValueError("Exactly three generator models are required.")
        if len(judge_models) != 3:
            raise ValueError("Exactly three judge models are required.")

        self.generator_models = generator_models
        self.judge_models = judge_models
        self.max_debate_rounds = max_debate_rounds

    # ------------------------------------------------------------------- #
    # 1Ô∏è‚É£ Generation round
    # ------------------------------------------------------------------- #
    def _generate_candidates(self, prompt: str) -> List[Candidate]:
        logger.info("üîß Generation round ‚Äì prompting %d models.", len(self.generator_models))
        candidates = []
        for idx, gen in enumerate(self.generator_models):
            source_id = f"generator_{idx}"
            try:
                content = gen(prompt)
                logger.debug("Model %s produced: %s", source_id, content[:100])
                candidates.append(Candidate(content=content, source_id=source_id))
            except Exception as exc:
                logger.exception("Generation failed for %s: %s", source_id, exc)
                raise
        return candidates

    # ------------------------------------------------------------------- #
    # 2Ô∏è‚É£ Blind judging round
    # ------------------------------------------------------------------- #
    def _blind_judge(self, candidates: List[Candidate]) -> Tuple[List[Judgment], bool]:
        """
        Returns a list of judgments and a boolean indicating whether consensus
        was achieved after this round.
        """
        # Assign anonymised IDs (A, B, C ‚Ä¶) ‚Äì deterministic order
        anon_ids = [chr(ord("A") + i) for i in range(len(candidates))]
        for cand, anon in zip(candidates, anon_ids):
            cand.anon_id = anon

        anon_texts = [cand.content for cand in candidates]
        logger.info("üîç Blind judging ‚Äì %d judges evaluating %d candidates.", len(self.judge_models), len(candidates))

        judgments: List[Judgment] = []
        for idx, judge in enumerate(self.judge_models):
            judge_id = f"judge_{idx}"
            try:
                ranking_indices = judge(anon_texts)  # expects list[int]
                ranking = [anon_ids[i] for i in ranking_indices]
                judgments.append(Judgment(judge_id=judge_id, ranking=ranking))
                logger.debug("Judge %s ranking: %s", judge_id, ranking)
            except Exception as exc:
                logger.exception("Judging failed for %s: %s", judge_id, exc)
                raise

        # Aggregate rankings via Borda count
        borda_scores: Dict[str, int] = defaultdict(int)
        for judgment in judgments:
            for position, anon_id in enumerate(judgment.ranking):
                # Higher score for higher rank (inverse of position)
                borda_scores[anon_id] += len(candidates) - position

        # Determine top candidate(s)
        sorted_scores = sorted(borda_scores.items(), key=lambda kv: kv[1], reverse=True)
        top_score = sorted_scores[0][1]
        top_candidates = [aid for aid, sc in sorted_scores if sc == top_score]

        logger.info("üó≥ Borda aggregation results: %s", dict(borda_scores))
        logger.info("üèÜ Top candidate(s): %s (score=%d)", top_candidates, top_score)

        consensus_reached = len(top_candidates) == 1
        return judgments, consensus_reached

    # ------------------------------------------------------------------- #
    # 3Ô∏è‚É£ Debate round (optional)
    # ------------------------------------------------------------------- #
    def _debate(self, candidates: List[Candidate], judgments: List[Judgment]) -> Candidate | None:
        """
        Runs a limited debate between the generator models. The debate is a
        simple iterative refinement where each generator receives the current
        top‚Äëranked solution and may produce an improved version.

        Returns the final consensus candidate or ``None`` if consensus could
        not be reached within the allowed rounds.
        """
        logger.info("üí¨ Debate phase initiated (max %d rounds).", self.max_debate_rounds)

        # Identify the current leading candidate(s) via Borda again
        anon_ids = [cand.anon_id for cand in candidates]
        borda_scores: Dict[str, int] = defaultdict(int)
        for judgment in judgments:
            for position, aid in enumerate(judgment.ranking):
                borda_scores[aid] += len(candidates) - position
        sorted_scores = sorted(borda_scores.items(), key=lambda kv: kv[1], reverse=True)
        leading_aid = sorted_scores[0][0]
        leading_candidate = next(c for c in candidates if c.anon_id == leading_aid)

        # Debate loop ‚Äì each generator gets a chance to improve the leading answer.
        for round_idx in range(self.max_debate_rounds):
            logger.info("üîÑ Debate round %d ‚Äì current leader: %s", round_idx + 1, leading_candidate.anon_id)

            # Feed the current best answer back to each generator
            improved_candidates: List[Candidate] = []
            for idx, gen in enumerate(self.generator_models):
                gen_id = f"generator_{idx}"
                try:
                    # Prompt format: original prompt + current best answer
                    refined_prompt = f"{leading_candidate.content}\n\nImprove or correct the above."
                    new_content = gen(refined_prompt)
                    improved_candidates.append(Candidate(content=new_content, source_id=gen_id))
                    logger.debug("Generator %s refined content: %s", gen_id, new_content[:80])
                except Exception as exc:
                    logger.exception("Debate generation failed for %s: %s", gen_id, exc)
                    raise

            # Re‚Äëjudge the refined set together with the existing leader
            all_candidates = [leading_candidate] + improved_candidates
            # Reset anon IDs for this sub‚Äëround
            for i, cand in enumerate(all_candidates):
                cand.anon_id = chr(ord("A") + i)

            # Blind judge again
            new_judgments, consensus = self._blind_judge(all_candidates)
            if consensus:
                # Return the new consensus candidate
                final_aid = new_judgments[0].ranking[0]  # top of any judge's list
                final_candidate = next(c for c in all_candidates if c.anon_id == final_aid)
                logger.info("‚úÖ Consensus reached during debate ‚Äì candidate %s", final_candidate.anon_id)
                return final_candidate

            # No consensus ‚Äì update leader based on Borda again
            # (reuse logic from _blind_judge without re‚Äëcreating objects)
            borda_scores = defaultdict(int)
            for judgment in new_judgments:
                for pos, aid in enumerate(judgment.ranking):
                    borda_scores[aid] += len(all_candidates) - pos
            sorted_scores = sorted(borda_scores.items(), key=lambda kv: kv[1], reverse=True)
            leading_aid = sorted_scores[0][0]
            leading_candidate = next(c for c in all_candidates if c.anon_id == leading_aid)

            # Store dissenting opinions for logging
            dissenters = [j for j in new_judgments if j.ranking[0] != leading_aid]
            if dissenters:
                logger.warning(
                    "üõë Round %d dissent ‚Äì judges %s preferred other candidates.",
                    round_idx + 1,
                    [d.judge_id for d in dissenters],
                )

        logger.warning("‚ùå Debate concluded without consensus.")
        return None

    # ------------------------------------------------------------------- #
    # Public API
    # ------------------------------------------------------------------- #
    def run(self, prompt: str) -> Dict[str, Any]:
        """
        Executes the full consensus protocol.

        Returns
        -------
        dict
            ``{
                "consensus": Candidate | None,
                "candidates": List[Candidate],
                "judgments": List[Judgment],
                "log": List[str]  # optional textual log extracted from logger if needed
            }``

        The caller can inspect ``consensus`` to see the final answer.
        """
        logger.info("üöÄ Starting consensus protocol for prompt: %s", prompt[:80])

        # 1Ô∏è‚É£ Generation
        candidates = self._generate_candidates(prompt)

        # 2Ô∏è‚É£ Blind judging
        judgments, consensus = self._blind_judge(candidates)

        if consensus:
            # Identify winning candidate
            winning_aid = judgments[0].ranking[0]  # all judges agree on top
            winning_candidate = next(c for c in candidates if c.anon_id == winning_aid)
            logger.info("üèÖ Immediate consensus achieved ‚Äì candidate %s", winning_candidate.anon_id)
            return {
                "consensus": winning_candidate,
                "candidates": candidates,
                "judgments": judgments,
                "log": [],  # placeholder ‚Äì real implementations may capture log output
            }

        # 3Ô∏è‚É£ Debate (if needed)
        final_candidate = self._debate(candidates, judgments)

        return {
            "consensus": final_candidate,
            "candidates": candidates,
            "judgments": judgments,
            "log": [],  # placeholder
        }


# --------------------------------------------------------------------------- #
# Example stub usage (removed in production; kept for reference)
# --------------------------------------------------------------------------- #
if __name__ == "__main__":
    # Simple mock models for quick local testing
    def mock_generator(prompt: str) -> str:
        return f"Answer based on: {prompt[:30]}..."

    def mock_judge(answers: List[str]) -> List[int]:
        # Na√Øve ranking: prefer the longest answer
        lengths = [len(a) for a in answers]
        return sorted(range(len(answers)), key=lambda i: lengths[i], reverse=True)

    node = ConsensusNode(
        generator_models=[mock_generator] * 3,
        judge_models=[mock_judge] * 3,
    )
    result = node.run("What is the capital of France?")
    print("Consensus:", result["consensus"].content if result["consensus"] else "None")