# Research: Evaluation and Metrics for AI Agent Systems

**Date:** 2026-02-03
**Scope:** Analysis of evaluation methods in the claude_parasite_brain_suck autonomous agent system
**Type:** Research document (no code changes)

---

## Executive Summary

This document analyzes the evaluation infrastructure in the current autonomous AI agent system, examining the critic-based quality scoring, performance tracking, self-verification mechanisms, and memory synthesis approaches. The system implements several research-backed techniques (LATS, TextGrad, Voyager, Generative Agents) but has significant gaps in evaluation coverage, multi-dimensional assessment, and outcome-based validation.

**Key Finding:** The current evaluation system relies heavily on single-critic self-evaluation with heuristic scoring. While functional, this creates systematic blind spots including self-evaluation bias, surface-level quality measures, and limited ground-truth validation.

---

## 1. Current Evaluation Methods

### 1.1 CriticAgent Scoring System (critic.py)

The system implements a **CriticAgent** class based on LATS (Language Agent Tree Search) and TextGrad patterns.

**Score Calculation:**
```
Base Score: 1.0
- Critical issues: -0.15 each
- Warning issues: -0.05 each
- Info issues: -0.02 each
Final: max(0.0, min(1.0, score))
```

**Quality Dimensions Evaluated:**
| Dimension | Check Method | Severity |
|-----------|-------------|----------|
| Error handling | Count try-except vs API calls | Warning |
| Import validity | Regex for unused imports | Info |
| Syntax basics | Bracket/brace matching | Critical |
| Pattern consistency | Logger, pathlib, JSON handling | Warning/Info |
| Logic issues | Empty functions, hardcoded values | Warning/Info |

**Pass Threshold:** Quality score >= 0.65

**Critic Retry Mechanism:**
- Triggered when quality_score < 0.7 and critic_mode enabled
- Maximum 2 retry attempts with TextGrad-style feedback injection
- Feedback includes specific improvement suggestions

### 1.2 Self-Verification (Voyager Pattern)

The `verify_grind_completion()` function implements Voyager-style self-verification:

**Verification Criteria:**
1. Exit code == 0 (successful execution)
2. Presence of success keywords in output: "done", "complete", "success", "finished", etc.
3. Evidence of files modified in output

**Verification Status:**
- PASS: Exit code 0 AND (success indicators found OR files modified)
- FAIL: Exit code != 0 OR (no success indicators AND no files modified)

### 1.3 Performance Tracking (performance_tracker.py)

Tracks quantitative metrics per session:
- `duration_seconds`: Execution time
- `success`: Boolean completion status
- `quality_score`: Critic-assigned score (0.0-1.0)
- `lessons_learned`: Extracted insights

**Derived Metrics:**
- Rolling averages (configurable window, default 10 sessions)
- Success rate percentage
- Improvement rate: `((current - baseline) / baseline) * 100`

### 1.4 Memory Importance Scoring (memory_synthesis.py)

Based on Generative Agents (arXiv:2304.03442):

**Importance Score Calculation (1-10 scale):**
```
Base: 5 (neutral)
+ High importance keywords: +2 each (critical, security, breakthrough, pattern, etc.)
+ Medium importance keywords: +1 each (fix, bug, implement, optimize)
- Low importance keywords: -1 each (check, list, status, minor)
Final: clamp(score, 1, 10)
```

**Retrieval Scoring:**
```
relevance_score = (importance_weight * 0.4) + (embedding_similarity * 0.6)
```

### 1.5 Error Categorization

Errors are classified into semantic categories for pattern detection:
- TIMEOUT: Session exceeded 600s limit
- ENCODING: Unicode/charset issues
- IMPORT: Module not found errors
- SYNTAX: Python syntax errors
- RUNTIME: General execution exceptions
- UNKNOWN: Uncategorized errors

### 1.6 Failure Pattern Detection (failure_patterns.py)

Tracks task failures with:
- Task similarity scoring (SequenceMatcher, 0.0-1.0)
- Characteristic overlap counting
- Warning levels: high (>= 0.85), medium (>= 0.75), low (>= 0.6), none

---

## 2. Evaluation Limitations

### 2.1 Self-Evaluation Biases

**Problem: The Critic Reviews Its Own Work**

The CriticAgent evaluates code generated by the same system (Claude). This creates several biases:

| Bias Type | Description | Evidence in Code |
|-----------|-------------|------------------|
| **Confirmation Bias** | Critic may favor patterns it would generate | Checks for "codebase patterns" but codebase was also AI-generated |
| **Blind Spots** | Critic can't catch errors it doesn't know to look for | Only checks predefined patterns (logger, pathlib, JSON handling) |
| **Optimism Bias** | Self-evaluation tends to overestimate quality | No external ground truth validation |
| **Style Over Substance** | Focuses on surface patterns, not actual correctness | Checks bracket matching, not logical correctness |

**Quantitative Concern:**
- The critic deducts only -0.15 per critical issue
- A file could have 2 critical syntax issues and still score 0.7 (passing)
- No penalty weighting for severity combinations

### 2.2 Quality Dimensions Missed

**Currently NOT Evaluated:**

| Missing Dimension | Why It Matters | Impact |
|-------------------|----------------|--------|
| **Semantic correctness** | Does code do what was asked? | Can pass all checks but wrong output |
| **Test coverage** | Are changes tested? | No validation of actual functionality |
| **Performance impact** | Is code efficient? | Potential resource waste |
| **Security depth** | Beyond pattern matching | Subtle vulnerabilities missed |
| **Code complexity** | Maintainability | Cyclomatic complexity ignored |
| **Documentation quality** | Understandability | Docstrings not validated |
| **Integration correctness** | Does it work with existing code? | No dependency graph analysis |
| **Edge case handling** | Robustness | Not evaluated |

**Critical Gap:** No validation that the generated code actually solves the original task.

### 2.3 False Positives and Negatives

**False Positives (Code passes but shouldn't):**
- Syntactically correct but logically wrong code scores well
- Code with subtle bugs passes bracket/import checks
- Unused imports only flagged as "info" (-0.02), barely affects score
- Hardcoded values only flagged as "info"

**False Negatives (Code fails but shouldn't):**
- Valid intentional empty functions (interfaces, stubs) flagged as "incomplete"
- os.path usage in legacy code flagged (pathlib not always appropriate)
- External calls without try-except may be intentional (caller handles)

**Verification False Positives:**
- Output containing "done" or "complete" anywhere passes
- Success keywords in error messages would pass verification
- Partial completion with any file modification passes

### 2.4 Threshold Brittleness

The system uses fixed thresholds:
- Pass threshold: 0.65 (arbitrary)
- Critic retry threshold: 0.7 (arbitrary)
- Reflection trigger: importance_sum > 150 (arbitrary)

**Problem:** No adaptive thresholding based on:
- Task complexity
- Historical performance
- Domain requirements
- Risk tolerance

---

## 3. Better Evaluation Approaches

### 3.1 Multi-Critic Ensembles

**Current State:** Single CriticAgent with heuristic scoring.

**Recommended Architecture:**
```
Task Output
    |
    +---> Style Critic (formatting, naming, patterns)
    |
    +---> Logic Critic (control flow, edge cases)
    |
    +---> Security Critic (vulnerability patterns)
    |
    +---> Performance Critic (complexity, efficiency)
    |
    v
Ensemble Aggregator --> Final Score + Detailed Report
```

**Ensemble Benefits:**
| Approach | Description | Expected Improvement |
|----------|-------------|---------------------|
| Weighted voting | Each critic contributes to final score | Reduces single-point-of-failure |
| Disagreement detection | Flag when critics disagree | Identifies uncertain evaluations |
| Specialist depth | Each critic focuses on one domain | Better coverage per dimension |
| Diverse perspectives | Different evaluation strategies | Reduces systematic bias |

**Implementation Considerations:**
- Cost: Multiple LLM calls per evaluation
- Latency: Parallel execution required
- Calibration: Critics must be calibrated to same scale

### 3.2 Rubric-Based Evaluation

**Current State:** Ad-hoc checks with implicit criteria.

**Recommended: Explicit Rubrics**

Example Task-Specific Rubric:
```
TASK: "Fix the login validation bug"

CRITERIA                          WEIGHT    SCORE (0-4)
----------------------------------------    -----------
1. Bug correctly identified         20%     [ ]
2. Fix addresses root cause         25%     [ ]
3. No regression introduced         20%     [ ]
4. Test case added/updated          15%     [ ]
5. Code style consistent            10%     [ ]
6. Documentation updated            10%     [ ]

TOTAL: (weighted sum) / 4.0 = Final Score
```

**Benefits:**
- Transparent: Everyone knows evaluation criteria upfront
- Task-specific: Different tasks have different success criteria
- Reproducible: Same rubric yields same evaluation
- Educational: System learns what "good" means

**Implementation:**
1. Task decomposition extracts rubric dimensions
2. Each dimension evaluated independently
3. Weights assigned based on task type
4. Final score is weighted aggregate

### 3.3 Outcome-Based Evaluation

**Current State:** Evaluates code quality, not actual outcomes.

**Recommended: Validate Actual Results**

| Validation Type | Method | What It Catches |
|-----------------|--------|-----------------|
| **Compilation/Parse** | Actually run compiler/parser | Syntax errors critic missed |
| **Test execution** | Run existing test suite | Regressions |
| **Behavior diff** | Compare before/after output | Functional changes |
| **Integration test** | Import and call changed code | Dependency issues |
| **Smoke test** | Basic functionality check | Complete failures |

**Implementation Hierarchy:**
```
Level 1: Static Analysis (current)
    |
    v
Level 2: Parse/Compile Validation
    |
    v
Level 3: Unit Test Execution
    |
    v
Level 4: Integration Test
    |
    v
Level 5: User Acceptance (manual)
```

**Key Insight:** A file that passes all critic checks but fails `python -m py_compile` is actually broken. Current system wouldn't catch this.

### 3.4 User Feedback Integration

**Current State:** No user feedback loop.

**Recommended Feedback Mechanisms:**

**Explicit Feedback:**
- Accept/reject task completion
- Rate quality (1-5 stars)
- Flag specific issues
- Provide correction examples

**Implicit Feedback:**
- Track which outputs are modified post-generation
- Monitor revert rates
- Measure time-to-first-use
- Track re-generation requests

**Feedback Integration:**
```
User Feedback
    |
    +---> Importance Score Adjustment
    |         (rejected outputs lose importance)
    |
    +---> Rubric Weight Tuning
    |         (frequently flagged criteria get more weight)
    |
    +---> Failure Pattern Recording
    |         (rejected outputs become negative examples)
    |
    v
Adaptive Evaluation System
```

**Implementation:**
1. Log all user interactions with outputs
2. Track outcome (accepted, modified, rejected)
3. Feed back into importance scoring
4. Adjust critic thresholds based on acceptance rates

---

## 4. Meta-Metrics

### 4.1 Learning Velocity

**Definition:** Rate of improvement in task success over time.

**Current Measurement:**
```python
improvement_rate = ((current_quality - baseline_quality) / baseline_quality) * 100
```

**Enhanced Metrics:**

| Metric | Formula | Interpretation |
|--------|---------|----------------|
| **Quality Velocity** | d(quality_score)/dt | Slope of quality over time |
| **Learning Curve Slope** | Linear regression on success rate | Steeper = faster learning |
| **Skill Acquisition Rate** | new_skills / time_period | How fast new capabilities emerge |
| **Plateau Detection** | Variance in recent scores | Low variance = plateau |

**Recommended Tracking:**
```
Session 1-10:  Baseline period (measure starting performance)
Session 11-N:  Track delta from baseline
Plot:          Quality score vs session number
Alert:         If 20-session moving average decreases
```

### 4.2 Knowledge Accumulation Rate

**Current Measurement:**
- `total_lessons_learned` count in performance_tracker
- Lesson count in learned_lessons.json

**Enhanced Metrics:**

| Metric | Measures | Calculation |
|--------|----------|-------------|
| **Unique Insight Rate** | Novel lessons per session | Deduplicated lessons / total sessions |
| **Reflection Density** | Higher-level synthesis | Level-2 reflections / total reflections |
| **Knowledge Utilization** | Are lessons actually used? | Lessons retrieved / total lessons |
| **Concept Coverage** | Breadth of knowledge | Unique concepts in KG / total nodes |
| **Knowledge Decay** | Lessons becoming stale | Lessons archived / lessons created |

**Retrieval Effectiveness:**
```
Lesson Utilization Rate = lessons_retrieved_and_applied / total_lessons
Target: > 30% (below indicates over-accumulation)
```

**Knowledge Graph Metrics:**
```
Graph Density = edges / (nodes * (nodes - 1))
Clustering Coefficient = connected_neighbors / possible_connections
Concept Specificity = 1 / avg(nodes_per_passage)
```

### 4.3 Cost Efficiency Trends

**Current Measurement:**
- Total cost summed from logs (cost_tracker.py)
- Cost per task (when logged)

**Enhanced Metrics:**

| Metric | Formula | Target Trend |
|--------|---------|--------------|
| **Cost per Success** | total_cost / successful_tasks | Decreasing |
| **Quality per Dollar** | avg_quality_score / cost | Increasing |
| **Retry Cost Ratio** | retry_costs / first_attempt_costs | Decreasing |
| **Model Efficiency** | (haiku_tasks * haiku_cost + sonnet_tasks * sonnet_cost) / total_cost | Stable |

**Cost-Quality Tradeoff Analysis:**
```
Pareto Efficiency: For each cost level, track max achievable quality
Optimal Operating Point: Where d(quality)/d(cost) starts diminishing
Budget Allocation: % to planning vs execution vs review
```

**Recommended Dashboard:**
```
+-------------------+-------------------+-------------------+
| Cost/Task: $0.08  | Quality/Task: 0.78| Success Rate: 82% |
| Trend: -3% /week  | Trend: +2% /week  | Trend: +5% /week  |
+-------------------+-------------------+-------------------+
| Retry Rate: 15%   | Model Mix:        | Budget Remaining: |
| Trend: -8% /week  | Haiku: 70%        | $185.50 / $200    |
|                   | Sonnet: 25%       |                   |
|                   | Opus: 5%          |                   |
+-------------------+-------------------+-------------------+
```

### 4.4 System Health Indicators

**Current Measurement:**
- Circuit breaker status (safety_killswitch.py)
- Error category distribution (grind_spawner.py)

**Enhanced Health Metrics:**

| Indicator | Description | Healthy Range |
|-----------|-------------|---------------|
| **Failure Clustering** | Are failures happening in bursts? | < 3 consecutive |
| **Error Diversity** | Single error type dominating? | No type > 40% |
| **Recovery Rate** | Failures followed by success | > 70% |
| **Timeout Frequency** | Sessions hitting time limit | < 5% |
| **Stuck Detection** | Same action repeated | < 2% of sessions |

**Anomaly Detection:**
```
Alert if:
- 5 consecutive failures
- Error category > 50% of recent failures
- Quality score < 0.5 for 3+ sessions
- Cost spike > 3x average
- Success rate drops > 20% in 10 sessions
```

**Health Score Composite:**
```
Health = 0.3 * success_rate
       + 0.2 * (1 - timeout_rate)
       + 0.2 * recovery_rate
       + 0.15 * quality_stability (1 - variance)
       + 0.15 * cost_efficiency
```

---

## 5. Research Recommendations

### 5.1 Near-Term Improvements (Low Effort)

1. **Add actual Python parsing validation**
   - Run `ast.parse()` on generated code
   - Catches syntax errors critic misses
   - Zero LLM cost

2. **Track user acceptance rate**
   - Log whether output was used/modified/rejected
   - Simple boolean feedback

3. **Implement quality score decay**
   - Recently successful outputs weight more
   - Prevents stale positive examples

4. **Add execution validation**
   - For code tasks, actually run the code
   - Compare output to expected behavior

### 5.2 Medium-Term Improvements (Moderate Effort)

1. **Multi-critic ensemble**
   - Add security critic (SAST patterns)
   - Add performance critic (complexity analysis)
   - Weighted voting aggregation

2. **Task-specific rubrics**
   - Define success criteria per task type
   - Auto-generate rubrics from task description
   - Weight by task requirements

3. **Outcome tracking**
   - Follow up on task results
   - Did the fix actually work?
   - Regression monitoring

4. **Adaptive thresholds**
   - Adjust pass threshold based on task complexity
   - Lower threshold for exploratory tasks
   - Higher threshold for production code

### 5.3 Long-Term Improvements (High Effort)

1. **Ground truth validation infrastructure**
   - Test suite integration
   - CI/CD pipeline hooks
   - Behavior diff tracking

2. **Human-in-the-loop calibration**
   - Periodic human evaluation of samples
   - Calibrate critic scores to human judgment
   - Adjust weights based on correlation

3. **Causal analysis**
   - What evaluation metrics predict actual success?
   - Which critic checks matter most?
   - Remove low-signal evaluations

---

## 6. Summary Tables

### Current Evaluation Coverage Matrix

| Quality Dimension | Evaluated? | Method | Confidence |
|-------------------|------------|--------|------------|
| Syntax validity | Partial | Bracket matching | Low |
| Import correctness | Partial | Regex heuristic | Medium |
| Error handling | Yes | Try-except counting | Medium |
| Pattern consistency | Yes | Codebase matching | Medium |
| Logical correctness | No | - | - |
| Test coverage | No | - | - |
| Performance | No | - | - |
| Security | Minimal | Pattern matching | Low |
| Documentation | No | - | - |
| Integration | No | - | - |

### Recommended Evaluation Priority

| Priority | Dimension | Effort | Impact |
|----------|-----------|--------|--------|
| 1 | Actual syntax validation (ast.parse) | Low | High |
| 2 | User feedback tracking | Low | High |
| 3 | Test execution validation | Medium | High |
| 4 | Multi-critic ensemble | Medium | Medium |
| 5 | Task-specific rubrics | Medium | Medium |
| 6 | Ground truth integration | High | High |

### Key Metrics Dashboard Recommendation

| Category | Metric | Current | Recommended |
|----------|--------|---------|-------------|
| Quality | quality_score | Yes | Add confidence interval |
| Quality | semantic_correctness | No | Add outcome validation |
| Velocity | improvement_rate | Yes | Add plateau detection |
| Knowledge | lessons_count | Yes | Add utilization rate |
| Cost | total_spent | Yes | Add cost-per-success |
| Health | success_rate | Yes | Add failure clustering |

---

## Appendix A: Evaluation Patterns from Literature

### LATS (arXiv:2310.04406)
- Tree search over solution space
- Value function estimates solution quality
- Reflection on failed paths
- **Key insight:** Evaluation guides exploration, not just filtering

### TextGrad (arXiv:2406.14762)
- Textual gradients for optimization
- Critic feedback as optimization signal
- Iterative refinement based on feedback
- **Key insight:** Evaluation should be actionable, not just scoring

### Voyager (arXiv:2305.16291)
- Self-verification separate from execution
- "Did we actually complete the task?"
- Environment feedback integration
- **Key insight:** Verify outcomes, not just outputs

### Generative Agents (arXiv:2304.03442)
- Importance scoring on all memories
- Reflection synthesis for higher-level insights
- Recency + Importance + Relevance retrieval
- **Key insight:** Not all evaluations equal; importance matters

---

## Appendix B: Evaluation Anti-Patterns Observed

1. **Single-point evaluation:** One critic, one score
2. **Surface-level checks:** Syntax but not semantics
3. **Fixed thresholds:** No adaptation to context
4. **No ground truth:** Never validate actual outcomes
5. **Self-grading:** System evaluates its own work
6. **Implicit criteria:** Success undefined until evaluation
7. **Binary outcomes:** Pass/fail without gradient
8. **No feedback loop:** User corrections not integrated

---

*End of Research Document*
