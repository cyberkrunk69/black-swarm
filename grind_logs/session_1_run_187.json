{"complexity": "complex", "subtasks": [{"id": 1, "assigned_to": "CODER", "description": "Scan the `experiments/` directory and produce a list of all experiment sub\u2011directories whose last\u2011modified timestamp is older than 24\u202fhours. Output the list to a file `old_experiments.txt` (one path per line).", "acceptance_criteria": "File `old_experiments.txt` exists and contains only directories older than 24\u202fh; no paths from the current day are present."}, {"id": 2, "assigned_to": "CODER", "description": "For each directory listed in `old_experiments.txt`, examine its contents: \n- Mark the directory as **useful** if it contains at least one non\u2011empty file.\n- Compute a SHA\u2011256 hash for every file and build a map of hash \u2192 directory list to detect duplicate content across experiments.\n- Produce two files: `useful_experiments.txt` (paths of useful dirs) and `duplicate_groups.json` (JSON mapping each hash to the list of directories sharing that hash).", "acceptance_criteria": "`useful_experiments.txt` lists only dirs with non\u2011empty files. `duplicate_groups.json` correctly groups directories that have identical file hashes; groups with a single entry can be ignored later."}, {"id": 3, "assigned_to": "CODER", "description": "Determine which directories should be removed:\n- Any directory from `old_experiments.txt` that is **empty** (no files) OR whose content hash appears in `duplicate_groups.json` with more than one directory (keep only the first occurrence).\n- Write the final deletion list to `to_delete.txt` and the keep list to `to_keep.txt`.", "acceptance_criteria": "`to_delete.txt` contains only directories safe to delete (empty or duplicate). No directory from today appears in either list. `to_keep.txt` contains the remaining unique, substantial experiments."}, {"id": 4, "assigned_to": "CODER", "description": "Delete all directories listed in `to_delete.txt` using a safe recursive removal (e.g., `shutil.rmtree`). Capture any errors and log them to `deletion_errors.log`.", "acceptance_criteria": "All paths in `to_delete.txt` are removed from the filesystem; the operation completes without raising uncaught exceptions. Any failures are recorded in `deletion_errors.log`."}, {"id": 5, "assigned_to": "CODER", "description": "Generate a summary report `prune_report.txt` stating:\n- Number of directories deleted (X)\n- Number of directories kept (Y)\n- Total bytes freed by the deletions (Z bytes, human\u2011readable format)\n- Any errors from `deletion_errors.log`.\nAlso include a short note confirming that no directories from the current day were touched.", "acceptance_criteria": "`prune_report.txt` exists, contains the required statistics, and clearly indicates successful completion with no accidental deletions of today\u2019s experiments."}], "next_role": "CODER", "critic_review": {"score": 1.0, "issues": [], "feedback": ["[OK] Code quality looks good!"], "passed": true, "timestamp": "2026-02-03T20:22:40.075030", "context": {"filename": "/app/grind_logs/session_1_run_187.json", "task": "PRUNE: experiments/ directory\n\nThere are 1824 experiment directories, most are garbage duplicates.\n\n1. List all experiments older than 24 hours\n2. Check which ones have actual useful output (non-empty, non-duplicate files)\n3. Delete empty or duplicate experiment directories\n4. Keep only experiments with unique, substantial output\n5. Report: X deleted, Y kept, Z bytes freed\n\nBE CAREFUL - don't delete anything from today.", "session_id": 1, "run": 187}}, "quality_score": 1.0, "critic_feedback": ["[OK] Code quality looks good!"], "critic_retry_count": 0}