{"complexity": "complex", "subtasks": [{"id": 1, "assigned_to": "CODER", "description": "Enumerate all sub\u2011directories inside `experiments/` and produce a list of those whose last\u2011modified timestamp is older than 24\u202fhours (i.e., not created/modified today). Output the list to a temporary file `old_experiments.txt` (one path per line).", "acceptance_criteria": "A file `old_experiments.txt` exists in the workspace root containing the absolute or relative paths of every experiment directory older than 24\u202fh. No directories from today are included."}, {"id": 2, "assigned_to": "CODER", "description": "For each directory listed in `old_experiments.txt`, inspect its contents and determine whether it contains any *useful* output. Useful output is defined as: (a) at least one non\u2011empty file, and (b) the file\u2019s content is not an exact duplicate of a file already seen in another experiment directory. Produce two files: `keep_experiments.txt` (paths to retain) and `delete_experiments.txt` (paths to remove). Also generate a JSON report `duplicate_map.json` mapping duplicate file hashes to the experiments that share them.", "acceptance_criteria": "`keep_experiments.txt` and `delete_experiments.txt` are created, mutually exclusive, and together contain all entries from `old_experiments.txt`. `duplicate_map.json` correctly lists SHA\u2011256 hashes of duplicate files and the experiment directories that contain each duplicate."}, {"id": 3, "assigned_to": "CODER", "description": "Delete all directories listed in `delete_experiments.txt` safely (recursively). Ensure the operation does not affect any directory not listed, especially those from today. Record the total number of directories deleted, total number kept, and total bytes freed (sum of sizes of removed files) in a plain\u2011text summary `prune_report.txt`.", "acceptance_criteria": "`prune_report.txt` exists and contains three lines: `Deleted: X directories`, `Kept: Y directories`, `Freed: Z bytes`. The filesystem reflects the deletions, and no directory from today has been removed."}, {"id": 4, "assigned_to": "CODER", "description": "Create a short README `prune_summary.md` that explains what was done, the criteria used, and points to the generated artifacts (`old_experiments.txt`, `keep_experiments.txt`, `delete_experiments.txt`, `duplicate_map.json`, `prune_report.txt`).", "acceptance_criteria": "`prune_summary.md` is present, well\u2011formatted in markdown, and accurately describes the pruning process and where to find the reports."}], "next_role": "CODER", "critic_review": {"score": 1.0, "issues": [], "feedback": ["[OK] Code quality looks good!"], "passed": true, "timestamp": "2026-02-03T20:19:19.395842", "context": {"filename": "/app/grind_logs/session_1_run_148.json", "task": "PRUNE: experiments/ directory\n\nThere are 1824 experiment directories, most are garbage duplicates.\n\n1. List all experiments older than 24 hours\n2. Check which ones have actual useful output (non-empty, non-duplicate files)\n3. Delete empty or duplicate experiment directories\n4. Keep only experiments with unique, substantial output\n5. Report: X deleted, Y kept, Z bytes freed\n\nBE CAREFUL - don't delete anything from today.", "session_id": 1, "run": 148}}, "quality_score": 1.0, "critic_feedback": ["[OK] Code quality looks good!"], "critic_retry_count": 0}