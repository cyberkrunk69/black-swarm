{"complexity": "complex", "subtasks": [{"id": 1, "assigned_to": "CODER", "description": "Traverse the `experiments/` directory and produce a list (e.g., JSON or CSV) of all experiment sub\u2011directories whose last\u2011modified timestamp is older than 24\u202fhours. Exclude any directory that was modified today.", "acceptance_criteria": "A file `experiment_list.json` is created containing an array of absolute paths for all qualifying experiment directories. The list must be reproducible and sorted alphabetically."}, {"id": 2, "assigned_to": "CODER", "description": "For each directory from the list, inspect its contents to determine whether it contains useful output. Useful output is defined as at least one non\u2011empty file that is not an exact duplicate of a file in another experiment directory (use file size and hash comparison).", "acceptance_criteria": "A new file `experiment_useful.json` is produced mapping each directory path to a boolean flag `has_useful_output`. Duplicate detection must be based on SHA\u2011256 hash of file contents."}, {"id": 3, "assigned_to": "CODER", "description": "Delete all experiment directories that are either empty or have no useful output according to the previous step. Ensure no directory modified today is touched.", "acceptance_criteria": "All directories flagged as not useful are removed from the filesystem. The script logs each deletion to `prune_log.txt` with timestamp and path. No directory with a modification date within the last 24\u202fhours remains deleted."}, {"id": 4, "assigned_to": "CODER", "description": "From the remaining directories, keep only one copy of each set of duplicate experiment outputs (i.e., if two directories contain identical non\u2011empty files, retain the newest one and delete the older duplicate).", "acceptance_criteria": "After execution, each group of duplicate experiment directories is reduced to a single directory (the most recent). Deletions are appended to `prune_log.txt` with reason \"duplicate\"."}, {"id": 5, "assigned_to": "CODER", "description": "Generate a final report summarizing the pruning operation: total number of directories deleted, total number kept, and total disk space reclaimed (in bytes). Write this report to `prune_report.txt`.", "acceptance_criteria": "`prune_report.txt` contains three lines: `Deleted: X`, `Kept: Y`, `Bytes Freed: Z`. The numbers must match the actual filesystem changes performed."}], "next_role": "CODER", "critic_review": {"score": 1.0, "issues": [], "feedback": ["[OK] Code quality looks good!"], "passed": true, "timestamp": "2026-02-03T20:17:08.927833", "context": {"filename": "/app/grind_logs/session_1_run_123.json", "task": "PRUNE: experiments/ directory\n\nThere are 1824 experiment directories, most are garbage duplicates.\n\n1. List all experiments older than 24 hours\n2. Check which ones have actual useful output (non-empty, non-duplicate files)\n3. Delete empty or duplicate experiment directories\n4. Keep only experiments with unique, substantial output\n5. Report: X deleted, Y kept, Z bytes freed\n\nBE CAREFUL - don't delete anything from today.", "session_id": 1, "run": 123}}, "quality_score": 1.0, "critic_feedback": ["[OK] Code quality looks good!"], "critic_retry_count": 0}