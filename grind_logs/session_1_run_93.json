{"complexity": "complex", "subtasks": [{"id": 1, "assigned_to": "CODER", "description": "Scan the `experiments/` directory and produce a list of all experiment sub\u2011directories whose last\u2011modified timestamp is older than 24\u202fhours (exclude any directory modified today). Save the list to a temporary file or in\u2011memory structure.", "acceptance_criteria": "A Python script (or shell script) that outputs a JSON or plain\u2011text list of qualifying experiment paths. The list must not include any directory whose modification date is within the last 24\u202fhours."}, {"id": 2, "assigned_to": "CODER", "description": "For each directory from step\u202f1, inspect its contents to decide if it holds \u201cuseful output\u201d. Useful output is defined as: (a) at least one non\u2011empty file, and (b) the set of files is not an exact duplicate of another directory\u2019s files (use file size and SHA\u2011256 hash to detect duplicates). Produce two collections: `keep_dirs` (unique, substantial) and `candidate_delete_dirs` (empty or duplicate).", "acceptance_criteria": "The script must correctly classify directories, producing two lists. Duplicate detection must be based on identical file hashes across directories. Empty directories (no files or only zero\u2011byte files) must be marked for deletion."}, {"id": 3, "assigned_to": "CODER", "description": "Delete all directories identified in `candidate_delete_dirs` while ensuring safety checks: never delete a directory whose name appears in the list from step\u202f1 that is less than 24\u202fhours old, and prompt (or log) before each removal for auditability.", "acceptance_criteria": "All directories slated for removal are permanently deleted from the filesystem. No directory newer than 24\u202fhours is touched. A log file records each deletion with timestamp and path."}, {"id": 4, "assigned_to": "CODER", "description": "Verify that the remaining directories (`keep_dirs`) contain unique, substantial output. Optionally, recompute hashes to ensure no duplicates remain after deletions.", "acceptance_criteria": "A final verification step confirms that no two kept directories share identical file sets. The script should output a confirmation message such as \u201cVerification passed: all kept experiments are unique.\u201d"}, {"id": 5, "assigned_to": "CODER", "description": "Generate a summary report showing: number of directories deleted (X), number kept (Y), and total bytes freed on disk (Z). Print the report to stdout and also write it to `prune_report.txt` in the project root.", "acceptance_criteria": "The report file `prune_report.txt` exists and contains lines like `Deleted: X`, `Kept: Y`, `Bytes freed: Z`. The numbers must match the actual filesystem changes performed by the script."}], "next_role": "CODER", "critic_review": {"score": 1.0, "issues": [], "feedback": ["[OK] Code quality looks good!"], "passed": true, "timestamp": "2026-02-03T20:14:24.458132", "context": {"filename": "/app/grind_logs/session_1_run_93.json", "task": "PRUNE: experiments/ directory\n\nThere are 1824 experiment directories, most are garbage duplicates.\n\n1. List all experiments older than 24 hours\n2. Check which ones have actual useful output (non-empty, non-duplicate files)\n3. Delete empty or duplicate experiment directories\n4. Keep only experiments with unique, substantial output\n5. Report: X deleted, Y kept, Z bytes freed\n\nBE CAREFUL - don't delete anything from today.", "session_id": 1, "run": 93}}, "quality_score": 1.0, "critic_feedback": ["[OK] Code quality looks good!"], "critic_retry_count": 0}